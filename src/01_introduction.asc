[[chap:introduction]]
= Introduction

The amount of information that is collected and processed today grows exponentially. It is estimated that by 2025 the global data volume will have reached 163 zettabytes, which is a trillion gigabytes cite:idc2017[]. The main drivers of this growth are social media, mobile devices, the Internet of Things (IoT), and the growing number of sensors built into various devices such as smartphones or (autonomous) cars.

A large part of the produced information can be located in time and place cite:Vatsavai2012[]. This kind of information is called _spatiotemporal data_ (or _geospatial data_, _geodata_). For many years, GPS technology has found its way into households, with location sensors built into consumer devices such as navigational systems or smartphones. These devices track their owner's position, record waypoints and routes, and save location information in every photo taken cite:Goodchild2007[]. In addition, earth observation satellites, as well as airborne laser scanners or terrestrial mobile mapping systems, offer similar data streams. Such devices record hundreds of thousands of samples per second cite:Cahalane2012[] and produce amounts of data ranging from a few GiB up to several TiB in a couple of hours cite:Paparoditis2012[].

Geospatial data can be of great value for a number of applications. For example, point clouds acquired by earth observation satellites can be used to regularly generate digital terrain models of large areas and to monitor changes in the landscape. This is useful for estimating the risk of landslides or for calculating the hydraulic energy produced by rain water running down steep terrain. In urban areas, geospatial data can be used for multiple use cases related to urban planning, environmental protection or disaster management. Data recorded by mobile mapping systems can be analysed to identify individual objects such as trees and to monitor their biomass for environmental protection.

Before the acquired geospatial data can be used in any of these applications it has to be processed. For example, point clouds generated by earth observation satellites need to be converted to a surface (i.e. triangulated to a digital terrain model), and the data acquired by mobile mapping systems in urban areas needs to be analysed to identify individual objects. The processing should happen in a reasonable amount of time, so that applications can make use of the most up-to-date information. However, there are inherent challenges related to geospatial data processing. citet:Yang2011[] differentiate between four factors of influence: _a)_ the high data volume, _b)_ the complexity of spatial processing algorithms, _c)_ the improving accuracy and better coverage of modern devices, as well as _d)_ the growing demand to share data and to concurrently access or process it for various purposes.

Due to this, geospatial data has been recognised as _Big Data_ cite:Kitchin2016[], which means it often exceeds the capacities of current computer systems in terms of available storage, computational power, as well as bandwidth. New distributed computing paradigms such as the Cloud address this issue. The Cloud is scalable, resilient, fault tolerant, and suitable for storing and processing growing amounts of data, while being responsive and centrally accessible. In recent years, it has become one of the major drivers of industry. Since hardware has become rather inexpensive and network connections have become faster--even over long distances--it is now possible to build large, high-performance clusters of commodity computer systems. The nodes in such Clouds can be used in concert to process large amounts of data in a very short time. Additionally, the cost for data storage is so low that Clouds can provide virtually unlimited space.

According to citet:nist2011[] from the U.S. National Institute of Standards and Technology (NIST) the Cloud model is composed of three service layers: _Infrastructure as a Service (IaaS)_, _Platform as a Service (PaaS)_ and _Software as a Service (SaaS)_. There are a number of vendors offering commercial platforms and targeting at least one of these layers. For example, Amazon Web Services (AWS), Google Cloud Platform or Microsoft Azure offer services in all three layers, whereas Salesforce.com or IBM Bluemix provide PaaS services and target customers who want to deploy their own SaaS solutions. In the geospatial domain large market players have only recently started to make use of the Cloud. Esri, the market leader for geospatial solutions, for example, offer first SaaS applications on AWS and Microsoft Azure. A wider use of the Cloud in the geospatial community is not observable yet, but there is a paradigm shift towards it which will lead to a general acceptance in the coming years.

== Paradigm shift towards GIS in the Cloud

Today, geospatial data is typically managed with desktop-based Geographic Information Systems (GIS) such as Esri ArcGIS or the open-source tool QGIS. The origins of GIS date back to the late 1960s, when the surveying community was faced with novel challenges stemming from the desire to use new sources of data and new techniques to analyse maps, as well as to be able to edit, verify and classify the data cite:Coppock199121[]. The first GIS ran on large mainframe computers controlled by punch-cards. With the advent of the personal computer in the 1980s, Geographic Information Systems became widely accepted, which leveraged the digitisation of the geospatial domain.

One of the first tools available to a broad audience was GRASS GIS, a free software initially developed by a number of federal agencies of the United States as well as private companies, with the aim to create a solution that could manage their growing catalogue of geospatial data sets. GRASS GIS is a modular system that consists of a number of individual command-line programs that can be called subsequently to perform custom spatial processing workflows. In the late 1990s, a graphical user interface was added to GRASS GIS, which allowed users to control the command-line programs and to display their results. At about the same time, Esri launched ArcGIS for Desktop which became the market leader for desktop-based GIS ever since. The introduction of graphical user interfaces in Geographic Information Systems was a major milestone that contributed to their broad success in the market.

Similar to the launch of GIS software and the implementation of graphical user interfaces, the geospatial market is now facing a new paradigm shift from desktop-based GIS to the Cloud. As described above, the Cloud offers many possibilities, in particular for the management of large data sets, but it is not yet widely used in the geospatial market. Although users increasingly face limitations with current solutions and the volume of geospatial data as well as the complexity of the processing algorithms exceed the storage and compute capabilities of their workstations, traditional desktop-based GIS offers a range of functionality that is not yet available in the Cloud. This not only applies to the number of spatial processing operations and algorithms the solutions offer, but also to the possibility to automate recurring work (or workflows) by creating scripts. For example, ArcGIS and QGIS allow users to create small programs in a general-purpose programming language such as Python. Automating recurring workflows can save time and money, but _a complete solution that offers a functionality similar to desktop-based products as well as the possibility to create workflows for the processing of geospatial data with a user-friendly interface does not exist yet in the Cloud_. In addition, current solutions based on general-purpose programming languages require expertise that users often do not have. Most of them have no background in computer science and do not want to deal with the technical details of workflow execution. In a distributed environment this issue becomes even more complex.

== Processing large geospatial data in the Cloud

The paradigm shift from desktop to the Cloud not only challenges users but also software developers who provide spatial operations and processing algorithms to Geographic Information Systems. A majority of these algorithms are very stable and have been tested in production for many years. However, since the algorithms were initially created for workstations, they are at best multi-threaded but not immediately suitable to be parallelised in a distributed environment such as the Cloud. In fact, most of the algorithms are single-threaded. In order to transfer them to the Cloud and to fully make use of the possibilities in terms of scalability and computational power, the algorithms need to be modified or completely rewritten--e.g. in MapReduce cite:Dean_Ghemawat_2008[] or a similar programming paradigm for distributed computing. In fact, many types of algorithms cannot be easily mapped and need to be completely redesigned.

Besides software developers in companies producing GIS solutions, there is a large scientific community with researchers who create state-of-the-art algorithms for geospatial processing. These researchers have different backgrounds such as mathematics, physics, photogrammetry, geomatics, geoinformatics, or related sciences. As such they are not computer scientists and have limited knowledge of programming of distributed applications. Executing their algorithms in the Cloud and making use of its full computational power are hard challenges for them. In fact, having to deal with the technicalities and characteristics of Cloud Computing prevents these researchers from focussing on their actual work--i.e. the creation of novel spatial algorithms.

Another challenge stems from the fact that MapReduce and similar programming paradigms allow for creating single distributed algorithms, but not for workflows that consist of a chain of algorithms. Researchers often work together with colleagues from other institutions and try to create processing workflows by combining algorithms they have developed independently. _At present, there is no workflow management system available that specifically targets geospatial data processing in the Cloud and that is flexible enough to be able to orchestrate and parallelise existing processing algorithms._

[[sec:introduction-problem-statement]]
== Problem statement

To summarise the challenges described above, we differentiate between two groups of people: _users_ of Geographic Information Systems, as well as _developers and researchers_ providing spatial operations and processing algorithms.

[role="mt-1"]
**Users** require

* an interface providing them with the means to process arbitrarily large geospatial data sets in the Cloud with the same set of operations and algorithms they know from their desktop-based GIS,
* the possibility to create workflows in order to automate recurring tasks and to execute them in the Cloud, as well as
* a user interface for workflow creation that does not require them to deal with the technical details of distributed computing or the Cloud infrastructure.

**Developers and researchers** require

* a way to execute their existing algorithms in the Cloud and to use its potential in terms of computing power and scalability, without having to fundamentally modify or re-implement their algorithms,
* an interface that allows them to integrate their algorithms without having to deal with the technical details of distributed computing such as parallelisation, data distribution and fault tolerance, and
* the possibility to orchestrate their algorithms and combine them with those from other parties in order to create complex processing workflows.

[[sec:introduction-objectives]]
== Objectives

In this thesis we aim to create a software architecture that addresses the challenges discussed in the previous sections. The architecture should assist both GIS users and developers in leveraging the possibilities of the Cloud. It should contain interfaces and extension points that allow developers to integrate their processing algorithms. Integration should not require fundamental modifications to the services. Instead, our architecture should be capable of parallelising existing algorithms (even single-threaded ones) and handling issues such as scalability and fault-tolerance without requiring the developers to have a deep knowledge of distributed computing.

Since the architecture should have the potential to replace a desktop GIS and to provide similar functionality in the Cloud, it should be modular so that many developers and researchers can contribute spatial operations and processing algorithms. These developers and researchers may work for various international companies and institutions that provide state-of-the-art components. The possibility to develop software artefacts in a distributed manner and to integrate them at a central place therefore plays an important role for the architecture.

The user interface of our architecture should allow users to create automated processing workflows for recurring tasks. It should be user-centric and hide unnecessary technical details, so that GIS users with no background in computer science can leverage the Cloud and overcome the limitations of their current workstations. Our architecture should be able to interpret the defined workflows and to orchestrate the algorithms contributed by the developers and researchers accordingly. Workflow execution should be scalable and utilise available Cloud resources to process arbitrary volumes of geospatial data.

== Hypothesis and approach

We formulate the following research hypothesis:

[quote]
____
A microservice architecture and Domain-Specific Languages can be used to orchestrate existing geospatial processing algorithms, and to compose and execute geospatial workflows in a Cloud environment for efficient application development and enhanced stakeholder experience.
____

The _microservice architecture_ is a style for designing software architectures where independent and isolated services act in concert to create a larger application. Each service (or _microservice_) runs in its own process and fulfils a defined purpose, similar to the geospatial processing algorithms described above. The architecture we present in this thesis is based on the microservice architectural style. As we will show later, this approach has significant benefits over the Service-Oriented Architecture traditionally used for distributed applications, in particular in terms of isolation of the services, as well as scalability and fault tolerance of the system. In addition, it offers the possibility to align the structure of the system to the organisational structure of the developing team and hence enables independent and distributed development. Since loose coupling is one of the core concepts, a microservice architecture can be easily extended and maintained. In our case this should allow us to reach our goal related to the integration of multiple processing algorithms contributed by distributed teams of developers and researchers and therefore enable efficient application development.

In order to enhance stakeholder experience, we will look at the requirements from users as well as developers. To orchestrate processing algorithms and to enable the execution of geospatial processing workflows, we will implement a component that works similarly to a scientific workflow management system. To integrate existing algorithms (or microservices) into our architecture we will present a novel way to describe the service interfaces in a machine-readable manner. Service execution and parallelisation in the Cloud will happen transparently to the developers who can therefore better focus on the algorithms. Finally, we will create a Domain-Specific Language (DSL) for the definition of workflows. A DSL is a small programming language targeted at a certain application domain. It is easy to understand for users from this domain, because it is based on vocabulary they are familiar with. Our Domain-Specific Language will have just enough elements to define a geospatial workflow. Its limited expressiveness will make it easier to learn and help users avoid common mistakes in distributed computing (such as concurrent write access to the same data set). In order to design the language, we will create our own modelling method which will be based on best practises from software engineering.

[[sec:intro-contributions]]
== Contributions

The contributions of this thesis to the scientific community are organised in three pillars. We present a _software architecture_ that contributes to the area of large geospatial data processing. This architecture contains a workflow management system for distributed _data processing_ in the Cloud. _Workflow definition_ is based on a Domain-Specific Language that hides the technical details of distributed computing from the users. The individual contributions of these pillars are described in the following in detail.

=== Architecture

The main contribution of this thesis is our software architecture for the processing of large geospatial data in the Cloud. It has the following major properties:

[role="mt-1"]
**Scalability.** The architecture supports the processing of arbitrarily large volumes of data. It makes use of available Cloud resources and can scale out (horizontally) if new resources are added. In one of the use cases we present later (see Section <<sec:introduction-use-cases>>) this will allow us to keep given time constraints and to process geospatial data as fast as it is acquired.

[role="mt-1"]
**Modifiability.** Our architecture is based on microservices. These services are loosely coupled and can be developed and deployed independently. This makes the architecture very modular and allows us to integrate various geospatial processing services which contribute to the overall functionality. The microservice architectural style provides good maintainability and helps create a sustainable system.

[role="mt-1"]
**Development distributability.** Distributed teams of developers and researchers with different backgrounds can work independently and create components that can be integrated into our architecture at a central location to build a single application. This enables us to extend the functionality of our system by state-of-the-art algorithms developed by international experts in geospatial processing.

[role="mt-1"]
**Availability.** Microservices are isolated components that run in their own processes and communicate over lightweight protocols. Due to this, our architecture has a high tolerance to the kind of faults that may happen in distributed environments. As we will show, our system is robust and continues to work if individual components fail. This also allows the distributed teams of developers to independently and continuously deploy new versions of their components without affecting system operation.

=== Processing

The second pillar of our thesis relates to distributed data processing and contributes to the fields of service orchestration and workflow management systems. Our main aim in this regard is to enable developers and researchers to leverage the possibilities of the Cloud for their own geospatial processing algorithms.

[role="mt-1"]
**Service integration.** We present a way to describe service interfaces (through service metadata) which is generic, lightweight, and covers a wide range of cases. This allows developers and researchers to contribute state-of-the-art processing algorithms to our architecture without requiring fundamental modifications.

[role="mt-1"]
**Service orchestration.** Our architecture contains a component called _JobManager_ which is a Workflow Management System. It converts user-defined workflows to executable process chains by orchestrating processing services. Based on the service interface descriptions, it is able to discover services and to create valid chains where outputs of services are compatible to the inputs of subsequent services. Service executions are parallelised if possible, without requiring service developers to implement specific features for distributed computing.

[role="mt-1"]
**Dynamic workflow management.** Our system supports dynamic workflows whose configurations can change during execution. We only require a priori runtime knowledge (see Section&nbsp;<<sec:processing-workflow-patterns>>). Other Workflow Management Systems require a priori design-time knowledge and can only execute static workflows where all variables have to be known before the workflow is started. Some of these systems offer workarounds for dynamic workflows, but we present an integrated approach.

[role="mt-1"]
**Rule-based workflow execution.** Our JobManager employs a rule-based system to convert workflows to process chains. The rules are configurable and can be adapted to various use cases as well as different executing infrastructures. The rules are also responsible for selecting services and data sets. In addition, they generate hints for our scheduler to distribute work to specific compute nodes in order to leverage data locality and to reduce network traffic.

=== Workflow modelling

The main aim of the third pillar of this thesis is to provide GIS users with the possibility to access the Cloud and to process large geospatial data without a deep knowledge of distributed computing. To this end, we provide a user-centric interface based on a Domain-Specific Language (DSL) which is a lean programming language tailored to a certain application domain. Specifically, we contribute to the scientific community in the following ways:

[role="mt-1"]
**DSL for workflow modelling.** We present a Domain-Specific Language for the processing of geospatial data. The language is modular and targets users from the domains of urban planning and land monitoring. It is easy to learn and--due to its limited expressiveness--prevents users with no IT background from making mistakes common to distributed computing such as concurrent write access to shared resources.

[role="mt-1"]
**Novel DSL modelling method.** In order to create our Domain-Specific Language, we present a novel incremental and iterative modelling method. This method makes use of best practises from software engineering as it encompasses domain analysis and modelling. These actions help identify relevant terms and actions for the Domain-Specific Language and ensure that the language is tailored to the analysed domain.

[[sec:introduction-research-design]]
== Research design

We follow a slight variation of the Design Science Research Methodology (DSRM). We create a solution for a defined problem and evaluate its utility and quality cite:Hevner2004[]. DSRM provides a nominal process model for doing Design Science research as well as a mental model for presenting and evaluating research cite:Peffers2007[]. Our method comprises the following steps:

[role="mt-1"]
**1. Problem identification and motivation.** Above, we have identified the problem of processing large geospatial data and motivated the creation of a software architecture. In addition, we perform a literature review for each of the three pillars we contribute to in our main Chapters&nbsp;<<chap:architecture>>, <<chap:processing>>, and <<chap:workflow-modelling>>. We compare existing work to our approaches and identify gaps.

[role="mt-1"]
**2. Define the objectives for a solution.** For the major objectives of our research we refer to Section <<sec:introduction-objectives>>. Following up on this, we formulate stakeholder requirements as well as quality attributes for our software architecture in Chapter <<chap:architecture>>. These requirements are derived from our work in various international research projects as well as our experience from developing large software systems and collaborating, over the last nine years, with domain users from municipalities, regional authorities, federal agencies, and the industry.

[role="mt-1"]
**3. Design and development.** We present our solution in our three main chapters. It consists of _a)_ the software architecture and components for _b)_ workflow-based data processing and _c)_ workflow modelling with Domain-Specific Languages. Each part of the solution has separate scientific contributions embedded in its design (see Section <<sec:intro-contributions>>).

[role="mt-1"]
**4. Demonstration and evaluation.** We carry out experiments based on two real-world use cases to demonstrate that our software architecture provides a solution to the formulated problem. These use cases are introduced in Section <<sec:introduction-use-cases>>. In Chapter <<chap:evaluation>> we perform a quantitative and a qualitative evaluation of our solution based on the formulated stakeholder requirements and quality attributes. We make use of scenarios which describe actors, stimuli, expected outcomes and response measures. We critically reflect each result and discuss strengths and possible weaknesses.

[role="mt-1"]
**5. Communication.** We have communicated our research results in various publications, extended abstracts, posters, and talks. A list of these can be found in Appendix <<app:publications>>.

[[sec:introduction-use-cases]]
== Use cases

According to our research design, we define requirements for our system based on our work in international research projects, the development of large software systems, and the collaboration with domain users over the last years. In order to evaluate our approach and implementation, we specifically focus on two use cases dealing with urban planning and land monitoring. Both use cases were formulated by GIS users within the IQmulus research project. They describe real-world scenarios with actual problems and goals.

IQmulus was a project funded from the 7^th^ Framework Programme of the European Commission, call identifier FP7-ICT-2011-8, under the grant agreement no. 318787, which started in November 2012 and finished in October 2016. The main aim of IQmulus was to create a platform for the fusion and analysis of high-volume geospatial data such as point clouds, coverages and volumetric data sets. One of the major objectives was to automate geospatial processing as much as possible and reduce the amount of human interaction with the platform. In the project we exploited modern Cloud technology in terms of processing power and distributed storage. As shown in Chapter <<chap:evaluation>>, we were able to use the results from this thesis successfully in this project.

[[sec:introduction-use-case-a]]
=== Use case A: Urban planning

The first use case describes typical tasks in a municipality or mapping authority. The GIS experts working there need to continuously keep cadastral data sets such as 2D maps or 3D city models up to date. They also perform environmental tasks such as monitoring the growth of trees. For this, the GIS experts make use of information from different sources including aerial images and LiDAR point clouds (Light Detection And Ranging) acquired by airborne laser scanning or laser mobile mapping systems (LMMS).

Figure <<img-stereopolis-ii-and-point-cloud>> shows the STEREOPOLIS II mobile mapping system as it is used by the national mapping agency of France, the Institut Géographique National (IGN), as well as a visualisation of a large 3D point cloud captured by this system on the Champs-Elysées avenue, Paris, France. The main challenges are the extraction of meaningful information from captured point clouds in an automated way and to handle the data volume and the velocity in which it is acquired. On a typical day of operation, STEREOPOLIS II generates hundreds of millions of points and several terabytes of data cite:Paparoditis2012[]. The average speed of the vehicle is 15 km/h. Within six hours it can cover about 90 linear kilometres. The captured point clouds are unstructured and unclassified. They contain raw geospatial coordinates and timestamps for each collected point. STEREOPOLIS II can be equipped with an image sensor to take panoramic high-definition images and to add colour information to the point clouds.

[#img-stereopolis-ii-and-point-cloud.top]
.The STEREOPOLIS II mobile mapping system by IGN (left) and a 3D point cloud acquired by the two upper RIEGL LiDAR devices (height coloured) over the Champs-Elysées avenue (right). Image source: citet:Paparoditis2012[]
image::images/01_introduction/stereopolis-ii-and-point-cloud.jpg[scaledwidth="100%",align="center"]

In the IQmulus project we worked together with end-users from the urban planning domain and identified the following user stories cite:iqmulus_d_1_2_3[]:

[quote]
____
**User story A.1:** As an urban planner, I want to capture topographic objects (such as cable networks, street edges, urban furniture, traffic lights, etc.) from data acquired by mobile mapping systems (LiDAR point clouds and images) so I can create or update topographic city maps.
____

[quote]
____
**User story A.2:** As an urban planner, I want to automatically detect individual trees from a LiDAR point cloud in an urban area, so I can monitor growth and foresee pruning work.
____

[quote]
____
**User story A.3:** As an urban planner, I would like to update my existing 3D city model based on analysing recent LiDAR point clouds.
____

[quote]
____
**User story A.4:** As an urban planner, I want to provide architects and other urban planners online access to the 3D city model using a
simple lightweight web client embedded in any kind of web browser, so that they are able to integrate their projects into the model and share it with decision makers and citizens for communication and project assessment purposes.
____

Note that user story A.4 describes a specific feature that was requested by users in the IQmulus project. Web-based visualisation of geospatial data is, however, not part of this work. We included this user story because it provides input to one of the examples we present in Chapter <<chap:workflow-modelling>> to demonstrate our modelling method for Domain-Specific Languages. Other than that, the user story is not considered any further in this work.

The user stories A.1 to A.3, on the other hand, describe the tasks discussed above. Municipalities and mapping agencies want to keep their data sets such as cadastral maps or 3D city models up to date. In addition, they need to monitor the growth of trees to coordinate pruning work. To this end, they analyse point clouds to identify building façades and individual objects such as traffic lights or trees. Since the point clouds are so large, the process should be completely automatic. Looking at the visualisation in Figure <<img-stereopolis-ii-and-point-cloud>>, with the human eye we can identify façades, two rows of trees and a couple of street items. If we just consider the vegetation, identifying _individual_ trees is, however, very challenging. Doing this in an automated way with a computer is even more so. This is due to the following reasons:

* Trees appear in a variety of sizes and shapes
* They are often only partially visible to the mobile mapping system
* Trees are located at different distances from the road, and may be close to façades, people, cars, street lights, other trees, etc.

There are existing geospatial processing algorithms addressing these issues cite:sirmacek2015,Monnier2012[]. Updating cadastral data sets and monitoring trees are continuous tasks that rely on up-to-date information, but the existing algorithms are very complex and applying them to a large data set can take a long time. The end-users from the IQmulus project reported that analysing the point clouds collected by the STEREOPOLIS II system takes much more time than the data acquisition. For example, a data set collected in the city of Toulouse, France within two hours, comprising more than 1.5 billion points with a total size of about 121 GiB took 52 hours of processing on a workstation that the end-users had access to. Considering that the STEROEPOLIS II system can typically operate for about six hours per day, continuously acquiring more data while the earlier data has not been processed completely reveals a major efficiency bottleneck. Keeping cadastral maps up to date and monitoring tree growth for a whole city is challenging, even on a weekly or monthly basis. The main obstacle of this use case is therefore to process large point clouds faster than they are acquired. In Chapter&nbsp;<<chap:evaluation>> we show that this is indeed possible with our architecture.

[[sec:introduction-use-case-b]]
=== Use case B: Land monitoring

The Liguria region in the north-west of Italy is a narrow, arch shaped strip of land bordered by the Ligurian sea, the Alps and the Apennine mountains. 65% of the terrain is mountainous, the rest is hilly. Some mountains rise above 2,000 m. The region's orography and its closeness to the sea contribute to the generation of complex hydro-meteorological events. There are a large number of drainage basins (or water catchments) that are connected in a hierarchical pattern (see Figure&nbsp;<<img-liguria-drainage-basins>>). During rainfall, water runs down from the mountains into these basins and subsequently into lower basins until it reaches the sea. This process creates considerable hydraulic energy. Heavy rainfall can cause floods, landslides, and in consequence, major environmental catastrophes. For example, in October 2011 there was an event with more than 468.8 mm of rain falling within 6 hours, with a maximum intensity of 143.4 mm per hour cite:DAmatoAvanzi2015[]. The water flooded three rivers and caused at least 658 shallow landslides. Thirteen people died during this event. The total cost was estimated at 1 Billion Euro. This kind of events occur on a regular basis. Other notable major events happened in November 2011 and two times in 2014 causing many deaths and considerable economic damage.

In order to better prepare against such events, the environmental department of the Liguria region ("`Regione Liguria`") needs to study orographic precipitation and understand the topography of the mountains in this area. Together with experts from this department, we specified the following user stories cite:iqmulus_d_1_2_3[]:

[quote]
____
**User story B.1:** As an hydrologist or a geo-morphologist supporting decision makers in civil protection, I want to analyse data measured during critical events to prepare better prediction and monitoring of floods and landslides.
____

[quote]
____
**User story B.2:** As an hydrologist, I want to study the evolution of measured precipitation data as well as slope deformation from optical images, compute parameters to produce high-quality input for hydrological and mechanical modelling and simulation, and compare the results to reference measurements obtained for flooding events and landslides.
____

The experts from the environmental department use LiDAR point clouds collected by airborne laser scanners. There are regular flights organised by the Italian Ministry of Environment to keep the data sets up to date and to study the evolution of the terrain over time. One such data set covers the whole Liguria region, has a high resolution and is therefore very large.

In this work we focus on the infrastructure and the parallelisation of the processing algorithms in order to speed up the process. The experts from the environmental department reported that a test on one of their workstations with initial versions of the processing algorithms took several days. In Chapter <<chap:evaluation>> we show that, due to our approach, the same process can be performed in about half an hour.

[#img-liguria-drainage-basins]
.Map of drainage basins in the Liguria region (randomly coloured)
image::images/06_evaluation/ls1/Ligura_basins_coloured.png[scaledwidth="100%",align="center"]

[[sec:introduction-relevant-publications]]
== Relevant publications

This thesis is partly based on previous, peer-reviewed work. In this section we describe how papers and project deliverables contributed to this thesis and specifically point out the advances we made since their publication. We also list works that did not contribute directly to this thesis but deal with similar topics or give further details on specific points. The list of publications is sorted by relevance.

citeproc_bibliography_adhoc::kraemer-senner-2015[]

// blank line
{zwsp}

In this journal paper we present a first version of our software architecture. The paper has contributed to Chapter <<chap:architecture>> but the text has been significantly updated and extended. This thesis includes a more detailed and elaborate description of the architecture, the components and their interfaces. In addition, we give a broader overview of the state of the art and describe how our work relates to it. Finally, we present a comprehensive requirements analysis that was not part of the original work. Although the paper included a few results from an initial evaluation, Chapter&nbsp;<<chap:evaluation>> is new and incorporates the advances we made since the publication of the paper.

// blank line
{zwsp}

citeproc_bibliography_adhoc::iqmulus_d_2_3_2[]

// blank line
{zwsp}

This deliverable from the IQmulus project also describes an earlier version of our architecture. It contributed some technical details to Chapter <<chap:architecture>>. The structure of the chapter is to a certain extent similar to the deliverable but the text has been significantly updated or rewritten. New sections have been added such as the comparison to the state of the art, the requirements analysis and the discussion on system operations.

// blank line
{zwsp}

citeproc_bibliography_adhoc::kraemer-2014[]

// blank line
{zwsp}

In this conference paper we present a modelling method for Domain-Specific Languages. Chapter <<chap:workflow-modelling>> is partly based on this earlier work. The text has been updated and new sections, such as the application of the modelling method to our use case B, were added.

// blank line
{zwsp}

citeproc_bibliography_adhoc::iqmulus_d_2_4_2[]

// blank line
{zwsp}

This project deliverable describes a Domain-Specific Language that is comparable to the one we present in Chapter <<chap:workflow-modelling>>. Our use cases are similar to the ones in the deliverable, but in this thesis we discuss related work in detail, we give a full overview over our grammar, and we describe our user interface (the workflow editor). In addition, we present a way to interpret workflow scripts written in the Domain-Specific Language and define how they can be mapped to executable actions.

// blank line
{zwsp}

citeproc_bibliography_adhoc::hiemenz-kraemer-2017[]

// blank line
{zwsp}

In this journal paper we present a method to store geospatial data securely in the Cloud, based on Searchable Symmetric Encryption. It contributed to Section <<sec:architecture-georocket>> on Cloud-based data storage and partly to Section <<sec:architecture-security>> on security.

// blank line
{zwsp}

citeproc_bibliography_adhoc::kraemer-frese-2017[]

// blank line
{zwsp}

This journal paper has been written in parallel with this thesis. It describes another software architecture based on microservices that enables secure Smart City applications in the Cloud. The paper has contributed to Section <<sec:architecture-microservice-architectures>> on microservice architectures and partly to Section <<sec:architecture-related-work-microservice-architectures>> on related work.

// blank line
{zwsp}

citeproc_bibliography_adhoc::boehm-bredif-gierlinger-kraemer-lindenbergh-liu-michel-sirmacek-2016[]

// blank line
{zwsp}

This conference paper gives further details on our use case A, in particular in terms of the algorithms used to process the urban data and the visualisation of the results.

// blank line
{zwsp}

citeproc_bibliography_adhoc::iqmulus_d_3_2[]

// blank line
{zwsp}

This deliverable from the IQmulus project is worth noting because it gives an overview of the processing chain (from interpreting workflow scripts written in a Domain-Specific Language to executing them in the Cloud).

== Structure of the thesis

The thesis is structured along the three pillars described in Section <<sec:intro-contributions>>. We start with a detailed description of our software architecture in Chapter <<chap:architecture>>. We include a comprehensive requirements analysis, interface descriptions, and a discussion on topics related to operations and security.

Chapter <<chap:processing>> presents details on our component for workflow execution. We describe interfaces as well as the internal control flow in the individual parts of our component. The chapter also includes a definition of service metadata which enables developers to integrate their services into our architecture.

The third pillar is covered by Chapter <<chap:workflow-modelling>> where we present our method for the modelling of Domain-Specific Languages as well as the language we use to describe workflows for our use cases. We also include a description of a user interface for workflow definition (a workflow editor) and describe how language elements can be mapped to executable actions.

In order to validate if our software architecture is suitable to execute workflows from real-world use cases, we present a comprehensive evaluation in Chapter <<chap:evaluation>>. We perform a quantitative evaluation where we apply our system to our use cases, as well as a qualitative discussion on the requirements defined in earlier chapters and how our system satisfies them.

We finish the thesis with conclusions and a discussion on future research.
