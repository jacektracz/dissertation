= Conclusions

With the availability of high-precision devices for mobile mapping and airborne laser scanning as well as the growing number of satellites collecting high-resolution imagery, the amount of geospatial data is becoming increasingly large. In addition, the number of consumer devices equipped with GPS sensors has rapidly grown in recent years and is expected to do so in the future. The same applies to stationary as well as mobile IoT devices.

This large amount of geospatial data needs to be processed in order to be useful for real-world applications. However, spatial algorithms are inherently complex and often require a long time to run. The data volume and the processing capabilities needed to process it exceed the capacities of workstations typically used in administrations, agencies, institutions and companies acting in the geospatial domain. Geospatial data has been recognised as Big Data, which imposes major problems for users of desktop Geographic Information Systems (GIS).

There is a paradigm shift in the geospatial community towards the Cloud which offers virtually unlimited space and computational power and is at the same time flexible, resilient, and inexpensive. However, the Cloud has not reached broad acceptance within the community yet. One of the major problems is that Cloud-based GIS products do not cover the functionality of their counterparts on the desktop. In addition, Cloud Computing is, in many aspects, still too complicated for GIS users, as well as developers and researchers providing spatial processing algorithms. The latter, in particular, are faced with new challenges regarding distributed computing but do not have the knowledge yet to tackle them.

== Research results

In this thesis we have presented a software architecture for the processing of large geospatial data which helps both GIS users and algorithm developers leverage the possibilities of Cloud Computing. The architecture is based on microservices. It is modular and allows developers to integrate existing processing services, even if they were not specifically designed to be executed in the Cloud. Efficient application development can be carried out in a distributed manner by developers and researchers who work independently and contribute their services to a centralised system. Our architecture is able to automatically deploy these services to compute nodes and to orchestrate them to distributed processing workflows, so that algorithm developers without an IT background do not need to learn concepts of distributed computing but can focus on the algorithmics. The processing workflows can be defined with an editor that is based on a Domain-Specific Language (DSL). This language is easy to learn for domain users and hides the technical details of distributed computing. This allows users to harness the capabilities of the Cloud and to focus on _what_ should be done instead of _how_. These benefits allow for an enhanced experience within the group of stakeholders, which includes users, developers, analysts, and managers.

In addition to the above, we have presented the results from a thorough quantitative and qualitative evaluation, in which we validated our architecture against stakeholder requirements and quality attributes. We were able to show that our approach satisfies all requirements. According to our research design described in Section <<sec:introduction-research-design>> we identified a problem, defined objectives for our research, presented a design artefact that serves as a solution for the problem, demonstrated and evaluated its utility and quality, and communicated our results in the scientific community.

_In summary, we can conclude that the results from our work provide evidence to support our research hypothesis:_

[quote]
____
A microservice architecture and Domain-Specific Languages can be used to orchestrate existing geospatial processing algorithms, and to compose and execute geospatial workflows in a Cloud environment for efficient application development and enhanced stakeholder experience.
____

== Contributions

The contributions of this thesis can be classified into three pillars. We have created a _software architecture_ contributing to the geospatial community and market by providing a means to process large data sets. In addition, we presented a workflow-based approach to the _processing of large geospatial data_ that contributes to the areas of workflow management systems and service orchestration. Finally, we described an approach to _workflow modelling_ based on a Domain-Specific Language and contributed our novel method for DSL modelling.

=== Architecture

Our software design is based on the microservice architectural style. Compared to monolithic applications, a microservice architecture is flexible and maintainable. In addition, it offers advantages over the Service-Oriented Architecture (SOA), in particular in terms of service deployment and execution. While services in an SOA may run in a single application container, microservices are isolated programs running in their own processes and serving a specified purpose. The high degree of isolation as well as the loose coupling of microservices helps create a distributed system in the following ways:

* *Scalability.* Our architecture has proven to be scalable in various dimensions. We were able to successfully integrate a high number of services and to deploy multiple instances of them to distributed compute nodes in the Cloud. Our architecture also allowed us to process large amounts of data and to increase performance by scaling out horizontally.

* *Modifiability.* Our system can be adapted to various use cases through the Domain-Specific Language and the rule-based system for workflow management. Loose coupling of microservices enables sustainable software development. We have shown that processing services from independent developers can be integrated to extend the functionality of our system.

* *Development distributability.* The microservice architectural style enabled distributed development. In the IQmulus research project we were able to implement a system based on our architecture that comprised more than a hundred services from twelve partners located in seven different European countries. Creating a stable system with such a range of functionality would have been impossible with a centralised, monolithic application.

* *Availability.* A microservice architecture allows for an easier implementation of various stability and availability patterns. We deployed multiple instances of our services redundantly to avoid Single Points of Failure (SPOF). In addition, we employed strategies such as the Circuit Breaker pattern to isolate services and to avoid cascading failures. The high degree of automation we used when deploying services, enabled us to keep our system constantly operating while we further developed it.

Maintaining a large distributed system can be challenging. For example, our architecture requires a certain degree of discipline from the processing service developers. We defined guidelines for service integration, but failing to follow them can undermine some of our concepts. Aspects such as fault tolerance or parallelisation can be affected if processing services do not work reproducibly and are not isolated. Additional discipline is required because our decentralised approach to create software requires service developers to not only be responsible for development, but also for deployment, operations, and user support. On the upside, this opens up new possibilities as developers can work more flexibly and independently without having to follow a strict release plan, for example. In addition, the loose coupling of microservices allows the developers to use technologies they are familiar with instead of being required to stick to a centralised technology stack.

High discipline is also required from the core system developers. A large microservice architecture can become very complex to handle. Distributed deployment of a large number of services, as is the case here, requires a high degree of automation. In addition, solutions for distributed logging and monitoring are mandatory in order to maintain an overview of the system, the deployed service instances and their current state. We only recommend a microservice architecture for systems that are large enough to justify the additional maintenance effort and that would be impossible to implement as a monolith. In our case, as stated above, the microservice architectural style was important to satisfy our requirements, and its benefits offset the challenges.

=== Data processing

The second pillar of this thesis was the workflow-based data processing. GIS users often try to automate recurring tasks by creating scripts in general-purpose programming languages with their desktop solutions. However, as mentioned above, the large amount of geospatial data exceeds the capacities of current workstations and the Cloud offers virtually unlimited storage space and computational power, but a system that is able to execute workflows for the processing of large geospatial data in the Cloud does not exist yet. More generic solutions for Big Data processing support creating single algorithms but not complex workflows.

* *Service integration.* We have worked with developers who had no background in computer science but were successfully able to integrate existing services into our architecture without having to learn distributed programming. Instead, they could focus on their algorithms. Our approach to service integration and orchestration is based on a lightweight interface description (service metadata). It is generic and does not require developers to implement a specific interface. Instead, they can describe how their services are called and integrate them as-is.

* *Service orchestration.* We have successfully implemented our approach to distributed workflow management. Our architecture is able to orchestrate geospatial processing services, to deploy them to Cloud nodes and to parallelise their execution, even if they were not specifically made for a distributed environment.

* *Dynamic workflow management.* We contributed to the state of the art with our approach to dynamic workflow execution without a priori design-time knowledge. Existing workflow management systems require all variables to be known before the workflow execution starts. However, geospatial applications often need a more dynamic solution. Very large data sets need to be split into smaller tiles that can then be processed in parallel on multiple compute nodes. At the end, the results need to be merged together. This pattern is visible in our use case B (land monitoring) where the input LiDAR strips are first partitioned along drainage boundaries and then processed independently. Later they are stitched together into a single 3D surface. The partitioning process can generate an indefinite number of tiles. The exact number will only be known during the workflow execution. With our architecture, we were able to successfully execute this kind of dynamic workflow.

* *Rule-based workflow execution.* Our approach to manage workflows is based on production rules. This makes our architecture configurable and adaptable to various use cases. For example, we can create rules that prevent users from accessing data sets or processing services if they do not have an appropriate license. The rule system can also be used to orchestrate services and to create executable process chains. Compared to other methods to describe service interfaces, our lightweight approach with generic service metadata does not define strict semantics for input and output ports. Our rule system can detect if the output of one service is not compatible to the input of a subsequent one (due to different file formats, different spatial reference systems, etc.) and add appropriate conversion services. In addition, our rule system can generate hints for our scheduler to leverage data locality and to reduce network traffic.

=== Workflow modelling

As mentioned in the previous section, desktop GIS products offer the possibility to automate recurring tasks with scripts written in a general-purpose programming language. GIS users need to understand this language and to have some experience in programming to make full use of it. If an automated task (or a workflow) should be executed in the Cloud, new questions arise, in particular regarding the selection of the right algorithms, the distribution of data, and the problem modelling.

* *DSL for workflow modelling.* With our approach to workflow modelling based on a Domain-Specific Language (DSL), GIS users with no background in computer science or distributed computing are enabled to automate tasks and process data in the Cloud.

* *Novel DSL modelling method.* In order to create such a language, we have used a novel approach to DSL modelling. This approach makes use of best practises from software engineering in order to derive the vocabulary for the language. It is incremental and iterative. Individual steps happen in collaboration with domain users in order to get direct feedback.

The DSL we created in this thesis covers two use cases from the urban and land domains. However, the basic vocabulary is generic and can be applied to other use cases too. The domain-specific terms are modular and can be replaced or extended depending on the actual requirements. Since human-readable names in our lightweight service metadata automatically become part of the DSL, the processing services that are integrated into an instance of our architecture define its appearance. This means if we would remove all processing services from our system that are specific to the geospatial domain and add services from another domain instead, we could use our architecture for completely new applications. If more specific expressions are required in the DSL we can apply our modelling method to this domain.

[[sec:conclusions-future-work]]
== Future Work

Given the potential of Cloud-based solutions for geospatial applications, we see many possibilities to continue our research and to improve our architecture. These possibilities cover aspects such as scalability and automatic data partitioning, as well as applying our solution to broader use cases. One aspect for further research was already mentioned in Section <<sec:architecture-georocket>> where we discussed the possibility to replace our distributed file system (DFS) by a structured and indexed storage solution for geospatial data called GeoRocket. The DFS has proven to be flexible and a good way to transfer files from one processing service to another. One problem that remains, however, is related to _data partitioning_. In our architecture we rely on the fact that the input data sets are already partitioned into smaller tiles and that these can be processed independently. This enables us to distribute the tiles to multiple compute nodes and to parallelise the processing. Problems arise if the individual tiles are too large to be handled by one node or if a certain algorithm requires multiple tiles to generate correct results--e.g. a feature extraction algorithm detecting shorelines that span across several tiles. In our use case dealing with land monitoring, the input data set is first repartitioned and indexed before it can be triangulated. A more intelligent tiling approach could be implemented with GeoRocket which is a data store that automatically splits imported data into smaller chunks. The indexing and querying features of GeoRocket enable a much more precise access to data. In future work we will investigate the use of this solution for our architecture and whether we can implement additional use cases with it. Since GeoRocket is a data store similar to an object storage solution such as AWS S3 with an HTTP interface, data access will be slower than with our distributed file system. We will investigate the performance impact and evaluate whether it is outweighed by the benefits.

Another area where we can improve the scalability of our architecture and contribute to the current state of the art in the area of workflow management systems is our _internal data model for workflows_. At the moment, we create a directed acyclic graph (DAG) to determine the dependencies between workflow tasks. This is an approach followed by many workflow management systems. The problem is that such a graph can become very large as soon as the number of files to be processed, and hence the number of tasks, grows. In our case, this problem is aggravated by the fact that we support dynamic workflows whose DAGs can change while they are executed. Such large graphs can grow beyond the size of the memory available to our JobManager. In addition, it can take a long time to traverse them. Existing workflow management systems try to solve this problem by offering the possibility to split a workflow into sub-workflows that have smaller DAGs and are executed subsequently. However, this is a workaround and the workflow management systems often leave it up to the user to decide whether to use it or not, which introduces additional complexity. We have already started working on a different approach that creates the graph incrementally based on the number of available compute nodes and memory cite:hellhake-2017[]. We will continue working on this approach as we see the potential to achieve high scalability with a low memory footprint.

Our approach to workflow management is based on a rule system. We were able to achieve a high flexibility and configurability with this but also observed some difficulties. With a high number of rules and many facts in the working memory, it can become hard to maintain an overview and it is easy for a developer to make mistakes and create rules that drastically affect the performance of the process chain generation--e.g. if a rule contains conditions that require the rule system to create a cross product of facts. In this work we were able to show that rules are suitable for the selection of data, processing services and nodes. In future work, we will try to focus on this and _reduce the complexity of our rule system in order to avoid performance pitfalls_. Many rules can be easily transferred to imperative code, which will improve the overall scalability and performance of our system without sacrificing configurability.

Finally, we will _investigate the use of our architecture for other applications in the geospatial domain_, in particular those where streams of data need to be processed continuously. For example, for land monitoring it might be suitable to regularly process satellite imagery that is produced every other week and to incorporate results from analysing daily weather data. Since our architecture is based on events, we can react immediately to newly imported data and start the processing in order to always be able to provide up-to-date results to the user. This approach can be combined with the data storage based on GeoRocket, which is also event-based and has a concept of a secondary data store containing processing results. We think such an approach provides a lot of potential for applications dealing with geospatial information, not only point clouds (such as in this thesis) but also raster images and spatio-temporal data.

== Final remarks

Before we started with research on the topic of processing large geospatial data about five years ago, the Cloud did not have the same market acceptance as today. This particularly applies to the geospatial market where the Cloud is still a niche solution and has only just begun to gain acceptance. The same is true for the microservice architectural style. Although microservices have gained a high momentum recently in the IT industry, they were relatively unknown five years ago. As we designed our architecture we needed an approach to create a large, Cloud-based system that consisted of many isolated services that are developed independently and in a distributed manner. The microservice architectural style became popular at almost the same time and, as such, helped us design our architecture.

Similarly, we see great potential for the Cloud in the geospatial market. In recent years we have observed a growing number of research projects dealing with Cloud-based geospatial applications. The same applies to the industry that offers more and more solutions in this area. In this thesis we have focussed on two groups, GIS users and developers of geospatial solutions. Although the Cloud has not reached broad acceptance in the geospatial market, we can observe that the users are increasingly becoming the market drivers. In particular, this is with respect to their demand to process large geospatial data that exceeds the capabilities of their workstations and that developers have started to meet this with innovative Cloud-based solutions.

The paradigm shift from desktop GIS to the Cloud is already taking place but there is still a long way to go. We believe that this thesis documents a major step, but more work needs to be done, not only on a conceptual or technical level, but in particular, in increasing the acceptance of the Cloud amongst users and developers.
