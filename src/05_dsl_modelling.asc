[[chap:workflow-modelling]]
= Workflow Modelling

In the previous chapters we focused on the Cloud architecture and how we can control the processing of large geospatial data in a distributed system. An additional challenge lies in providing the user or domain expert with means to control the data processing in an easy and understandable manner.

In this chapter we present a way to define a processing workflow in a _Domain-Specific Language (DSL)_. A DSL is a lightweight programming language tailored to domain users with little to almost no IT background. Such a language uses the vocabulary of a specific domain and is hence easy to understand and learn for people working in this domain.

The chapter is structured as follows. First, we motivate the use of Domain-Specific Languages for distributed geospatial data processing and give an overview of related work. We then present a novel method for DSL modelling, followed by two example use cases demonstrating how it can be applied in practise. After that, we describe a technique to map DSL expressions to processing services and datasets in the Cloud. The chapter ends with a summary and conclusion.

== Introduction

Current desktop-based GIS solutions are feature-rich and suitable for a wide range of geospatial applications. The commercial software ArcGIS by Esri and the Open-Source tool QGIS, for example, are among the most popular desktop GIS solutions and used by many companies, municipalities and other institutions. Whenever users need more flexibility than these products already offer, they can develop scripts and extensions. This also allows them to automate recurring workflows which have otherwise to be performed manually. The scripts and extensions are usually written in a general-purpose programming language such as Visual Basic, C++ or Python. These languages provide high flexibility, but they also require the users to have experience in programming. GIS users are experts in their domain but typically have no background in computer science. Writing scripts is often a complex and tedious task for them.

This problem becomes even more complex in a Cloud Computing environment where users are faced with additional questions such as the following:

* Which algorithms do I have to use? Which algorithms can actually be executed in my Cloud environment?
* How do I model my problem so it can be solved with a parallelised algorithm?
* Does my data have to be partitioned in order to be processed in parallel in the Cloud? How small or large do I have to make the chunks in order to process them efficiently and to exploit the processing capabilities of the Cloud as much as possible (granularity)?
* How do I have to store my data so the parallelised algorithm can access it efficiently? Where do I have to store it (i.e. in which virtual data centre)?
* etc.

Current Cloud technologies give help on these questions only to a certain degree. Users are mostly required to find solutions on their own. Approaches such as MapReduce cite:Dean_Ghemawat_2008[] allow for processing large amounts of data, but defining a MapReduce job--let alone a chain of several jobs that make up a complete workflow--requires a deep understanding of the processed data, the algorithms, and the infrastructure (i.e. the Cloud environment). Additionally, users have to have a background in computer science and software development, in particular functional programming.

Users dealing with geospatial data processing need to be able to define high-level workflows that focus on the actual problem and not the technical details of the underlying, executing system. The workflow definition language should not be a general-purpose programming language requiring comprehensive programming skills. Instead, it should be designed for a single application domain, which means it should consist of a reduced vocabulary that is tailored to the tasks the domain expert needs to perform. In particular, such a language should have the following characteristics:

* It should be tailored to describe workflows (and nothing else).
* It should be tailored to a certain application domain--in this case geospatial processing, or even more specific, data processing in urban planning (see Section <<sec:dsl-example-use-case>>), land monitoring (see Section <<sec:dsl-use-case-b>>).

A language that is meant to be used in a specific application domain is called _Domain-Specific Language_ or _DSL_. According to citet:Fowler_2010[] there are two kinds of DSLs: internal and external ones. _Internal DSLs_ are based on another programming language (the host language) which is typically a general-purpose language. In its simplest form an internal DSL is nothing more than a well-designed API like a fluent interface that makes it easy to express a complex matter in a readable way. More advanced internal DSLs make use of special features of the host language such as AST transformations--in the case of Groovy DSLs for example cite:Dearle2010[]&mdash;or operator overloading and implicit values--see Scala DSLs cite:Hofer2010[]. These features allow language developers to create an internal DSL that looks like a completely new programming language, but reuses components of the host language such as its lexer and parser cite:Hudak1998[]. Internal DSLs are very powerful because they are simple and easy to use, but also allow their users to fall back to using the host language if they need more advanced features. A good example for this is the Domain-Specific Language interface of the build automation tool Gradle cite:gradle2017[]. Almost all tasks necessary to specify a complex build process can be expressed with Gradle's internal DSL. Whenever users need more flexibility, they can write code in the host language Groovy.

An _external DSL_, on the other hand, can be designed independently of existing languages. It has a separate syntax and grammar and requires a new interpreter or compiler. External DSLs do not offer the possibility to fall back to features of a host language and are often not as powerful as internal ones. Nevertheless, their limitations enable the interpreter or compiler to produce optimised code. For example, a language that does not offer a way to change the value of a variable or to do I/O operations may be better suited for writing a multi-threaded program than an imperative general-purpose language where developers need to take special care of possible side effects.

A comprehensive survey on Domain-Specific Languages is given by citet:Mernik_Heering_Sloane_2005[]. They describe patterns in the decision, analysis, design, and implementation phases of DSL development and try to give support to developers on when and how to develop such languages. One of the issues they discuss is the question of whether a language designer who wants to solve a particular problem should create an internal DSL (or _embedded DSL_) or if an external DSL is a more suitable approach. In this regard, they state the following:

[quote]
____
If the DSL is designed from scratch with no commonality with existing languages (invention pattern), the recommended approach is to implement it by embedding, unless domain-specific analysis, verification, optimization, parallelization, or transformation (AVOPT) is required, a domain-specific notation must be strictly obeyed, or the user community is expected to be large.
____

In this work we aim for creating a Domain-Specific Language that is easy to learn for non-IT personnel and that requires no background in programming. We therefore want to design a new language that does not resemble existing general-purpose programming languages. This is what citea:Mernik_Heering_Sloane_2005[] call the _invention pattern_. According to them, embedding is the recommended approach for this. Consequently, we should design an internal DSL. However, citea:Mernik_Heering_Sloane_2005[] state that an external DSL is more suitable in at least one of the following cases:

* If _domain-specific analysis, verification, optimisation, parallelisation, or transformation (AVOPT)_ is required. In this work we want to create a system that processes geospatial data in a distributed manner in the Cloud. Our JobManager applies domain-specific analysis as well as optimisation and parallelisation to schedule the execution of processing services. As discussed above, this would be barely possible if we allowed access to all features of a general-purpose language. The limitations of an external DSL actually enable the JobManager to calculate a suitable execution plan.

* If a _domain-specific notation_ must be strictly obeyed. This is not required by our use cases but it is beneficial as it makes the language more readable and easier to learn for domain experts.

* If the _user community_ is expected to be large. In this work we target GIS experts from various organisations, municipalities and institutions. We cannot assume that the users will have a background in programming. citea:Mernik_Heering_Sloane_2005[] specifically state that error reporting is a problem with internal DSLs as the error messages depend on the host language compiler and cannot be customised or are in the worst case misleading. A separate interpreter or compiler for an external language, on the other hand, can be customised to produce domain-specific error messages that can be understood by a wide range of users.

In this sense, we focus on external DSLs. This allows us to design a language independently of any implications of a host language. It also enables us to create a DSL that is tailored to describing workflows, but does not offer any features that would prevent execution in a distributed environment. In the following, the term _DSL_ is hence used synonymously with _external DSL_.

citet:Mernik_Heering_Sloane_2005[] refer to a large number of earlier articles and argue that while many of them focus on the development of particular DSLs there is almost no work on DSL development methods. In Section <<sec:modelling-method>> we follow up on this matter and describe a method for DSL modelling. We apply it in Section <<sec:dsl-example-use-case>> to an example use case to create a language tailored to the definition of geospatial processing workflows.

== Related work

In this section we discuss related work on Domain-Specific Languages, in particular their use in software development and Cloud Computing. We also compare textual languages with visual programming languages and discuss methods to model and develop DSLs.

=== Domain-Specific Languages in software development

Domain-Specific Languages have a history in software development. They are used in various areas of software systems, in particular in those that depend on user requirements, business workflows, or on the environment where the system is deployed. Figure <<img_fractal_programming>> depicts the layers of code in a complex software architecture, each of them being defined by their dynamics and programming languages used cite:Bini_2008[prefix=see].

[[img_fractal_programming]]
.Layers of code in a complex software architecture
image::images/06_dsl_modelling/fractal_programming.pdf[scaledwidth="40%",align="center"]

The stable layer contains code that rarely changes. It is usually written in a statically typed language such as Java or C++ and contains the application's kernel. The second layer changes more often. Here, developers use scripts written in dynamically typed languages such as Ruby or Groovy to affect the application's behaviour and to customize it for a certain group of users or customers. In the third layer, the domain layer, users can change the application's behaviour directly through scripts written in Domain-Specific Languages. This layer is the one that changes most often. It is directly connected with the users`' requirements and the tasks they need to perform in their respective application domain.

The advantage of using dynamic languages in the second layer is that developers do not have to recompile the whole application if their workflow has changed or if the application should be used for another customer working in the same area but having slightly different requirements. The code in both the dynamic layer and the static layer is typically written by software developers and not end-users. The third layer, on the other hand, is about the domain the application is used in. Even in the same domain, users often need to perform various tasks with different workflows and requirements. For example, in the geospatial domain, as described earlier, users need to process heterogeneous geodata with a range of algorithms and processes. It would be too expensive and time-consuming to ask the application developers to extend the system every time a new task has to be done. Instead, in the domain layer, the users define workflows on their own. Obviously, they need an easy and understandable interface for that. This is why in the third layer Domain-Specific Languages are used, which are tailored to the tasks the users have to perform in their application domain.

=== Domain-Specific Languages for data processing

The use of DSLs in the area of Cloud Computing to control distributed processing has been
demonstrated by citea:Olston2008[] who present the software library Apache Pig and in particular its high-level processing language Pig Latin cite:Olston2008[]. The language looks a lot like the database query language SQL--which is in fact also a Domain-Specific Language--so users who are familiar with SQL are able to quickly learn and use Pig Latin as well. For example, the following script loads a 3D point cloud including information about classification represented by a number specifying if a point has been classified as belonging to the ground, a building, a tree, or if the classification was inconclusive (in this case the number will equal `-1`). The script removes points that are not classified and then groups the remaining points by their classification.

[source,subs=+quotes]
----
point_cloud = *LOAD* _'point_cloud.xyz'_ *USING* PigStorage(_','_)
    *AS* (x: *double*, y: *double*, z: *double*, classification: *int*);
filtered_points = *FILTER* point_cloud *BY* (classification != -1);
grouped_points = *GROUP* filtered_points *BY* classification;
*DUMP* grouped_points;
----

The language drives distributed MapReduce jobs executed in the Cloud by the Apache Hadoop framework. Pig Latin simplifies the process of specifying complex parallel computing tasks which can sometimes be tedious even for experienced programmers. However, it is very generic and does not target a specific application domain like the language we aim for in this work. In addition, it is not really a workflow description language but can be better compared to a query language. It requires exact knowledge about the structure of the data to process.

Pig as well as Pig Latin lack support for geospatial processing. This gap is closed by the SpatialHadoop framework which adds spatial indexes, as well as geometrical data types and operations to Apache Hadoop cite:Eldawy2013[]. In addition, SpatialHadoop offers a DSL based on Pig Latin. Pigeon is a high-level query language that allows users to specify queries that operate on geospatial data cite:Eldawy2014[]. However, since Pigeon is based on Pig Latin it shares the same properties and is therefore quite different to the language that we design in this work.

Domain-Specific Languages have also been applied successfully in the area of parallel computing. citet:Chafi2011[] present Delite, a compiler framework and runtime for high performance computations. They specifically target multi-core systems as well as GPGPUs (general-purpose computing on graphics processing units). Their approach has been successfully applied to OptiML which is a Domain-Specific Language for high-performance parallel machine learning cite:Sujeeth2011[]. DSLs developed with Delite are embedded into the Scala programming language. The advantage of this approach is that existing compiler components can be reused (Delite acts as a compiler plug-in) which substantially reduces the amount of work for the language designer. However, language users are required to have at least some experience in programming, in particular Scala whose syntax--which sometimes is referred to as being complicated and hard to read cite:Juric2010,Ulrich2012[]&mdash;cannot be completely hidden in the DSL. In this work we develop an external DSL instead that is not coupled to a general-purpose programming language and whose syntax can therefore be designed freely.

Another area where DSLs have been used for data analysis and processing is urban planning. In a previous work we presented a Domain-Specific Language for urban policy modelling cite:kraemer-ludlow-khan-2013[]. The language offers the possibility to specify policies and policy rules in a formalised and machine-readable way. At the same time the policy rules stay clearly readable and understandable for urban planners and decision makers. The following example shows a policy rule and a production rule that changes a visualisation on the user's screen when the analysed data indicates the policy rule was not met.

[source,subs=+quotes]
----
The number of cars on street B per day
    has to be lower than 2500.

When the number of cars on street B per day
    is higher than 2500
then display street B in red.
----

We further elaborated the idea of using production rules to process geospatial datasets in another one of our previous work cite:kraemer-stein-2014[]. We presented a graphical DSL that can be used to specify typical spatial processing workflows in the urban planning domain. The graphical user interface has been integrated into a 3D GIS (Geographic Information System). User evaluation carried out in the urbanAPI research project funded by the European Commission confirmed that such a DSL is indeed useful, but more work is needed, in particular since the language is not designed to operate in a Cloud Computing environment cite:kraemer-2014[].

=== Textual and visual programming languages

The workflow management systems currently available offer different ways to define workflows. While systems such as Pegasus rely on a textual programming language, others such as Kepler or Taverna provide a graphical user interface. In addition, there are a couple of workflow languages available (see Section <<sec:dsl-workflow-description-languages>>) such as YAWL (Yet Another Workflow Language), CWL (Common Workflow Language) or BPEL (Business Process Execution Language) for which software vendors have developed textual or graphical editors.

For the user interface in this work we have considered both textual and graphical ways to define workflows. We based our decision to design a textual DSL instead of a graphical editor on the comparison of general textual programming languages (TPL) and visual programming languages (VPL). citet:whitley-1997[] presents a survey on the state of the art in VPLs and tries to give empirical evidence for and against them. He reviews a large number of publications and compares their results in order to decide whether TPLs or VPLs are superior to each other. He concludes that visual programming languages can improve usability and help users perform specific tasks, but this does not apply in all cases. The same is true for textual programming languages. The reason for this can be attributed to the _match-mismatch hypothesis_ described by citet:gilmore-1984[]. They state that every notation highlights some kinds of information while it hides others. This means that while a notation might be suitable for a certain task, it will fail for others. The match-mismatch problem implies that for every task different kinds of notations have to be used cite:ruppert-dambruch-kraemer-2015[]. This is supported by citet:brooks-1987[] who claims there is "`no silver bullet`"&mdash;i.e. no "`development [...] that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity.`" He also discusses earlier research in visual programming languages and states: "`Nothing even convincing, much less exciting, has yet emerged from such efforts. I am persuaded that nothing will.`"

In addition to the match-mismatch hypothesis, citet:whitley-1997[] also discusses the role of primary and secondary notation. He states that the primary notation--i.e. the syntax and grammar of a TPL or the visual elements of a VPL--is of major importance for the comprehensibility of a language, but the secondary notation--the way elements are presented on the screen--also plays a major role. The secondary notation of textual programming languages is the way the text is formatted--i.e. how the lines are indented or the text is presented (bold, italic, underlined, etc.). In visual programming languages the secondary notation is the way visual elements (boxes, arrows, etc.) are placed on the screen. In both cases, the secondary notation influences usability. A text that is well formatted or a visual representation whose elements are well placed can help users understand a program, but badly formatted text and misplaced visual elements can even confuse them.

In our experience it is much harder for untrained users to create a clear and tidy visual representation than it is to write a well-formatted textual program (especially if the matter to express is complex). This is supported by citet:booth-stumpf-2013[] who present an exploratory empirical study on the end-user experience with visual and textual programming environments for the physical computing platform Arduino. They investigate how adult end-user programmers--non-professional programmers with no experience in the Arduino platform but basic knowledge of programming--can perform a number of tasks in a textual and a visual environment. They conclude that the study participants managed to finish the tasks better in the visual programming environment. However, they also note that the tasks where they performed better were mostly related to modifying an existing program instead of creating a new one. The visual layout was already there and the learning barriers were therefore much lower.

Another study on the comprehensibility of visual and textual programs is presented by citet:green-petre-1991[]. They explore the fallacy many people make by assuming a VPL is better just because it is visual. They call this phenomenon "`Superlativism`" because people often think that visual perception is more efficient than reading text in general, while it can be shown that some tasks benefit from visual programming languages and others do not. They conclude that (at least in their study) VPLs performed worse compared to TPLs. Gilmore and Green's match-mismatch hypothesis proved correct again because they were not able to find a single VPL with which they could cover all possibilities in their experiment. Their study is further supported by citet:petre-1995[] who explains "`Why looking isn't always seeing`" and questions correctness of the English idiom "`A picture is worth a thousand words`". He presents figures of identical programs in visual and textual form and explains in which aspects they succeed and in which they fail. He also discusses the importance of a language's secondary notation.

citet:neag-tyler-kurtz-2001[] investigate visual and textual programming in automatic testing. They list a number of aspects that limit the use of VPLs:

* _Visualization restriction._ A computer screen has a certain size and the number of elements it can display is limited. Complex visual programs can become very hard to read if users have to scroll.

* _Limited modularization capabilities._ Textual programming languages typically provide means to structure a program into namespaces, modules, packages, objects, etc. Many visual programming languages lack these capabilities.

* _Limited support for embedded documentation._ TPLs allow comments to be added in the source code to document the program's behaviour.

* _Limited ability to model complex data structures and to describe complex algorithms._ It is hard to express both data flow and control flow in a single visual program.

In addition, we note that unless a standardised modelling language such as BPMN is used, a visual programming language binds the user to a specific software solution (vendor lock-in). Textual programs can easily be transferred from one system to another and can be edited in any text editor while visual programs require a specific modelling tool.

In summary, we can conclude that there is no clear evidence whether a TPL or a VPL is superior or not. There are many studies proving that a domain-specific representation helps users in performing certain tasks. Visual programs often perform better when the tasks are well defined and their extent is limited, but fail otherwise. Visual programs suffer from the problems described by the match-mismatch hypothesis as much as textual ones. They benefit from a good secondary notation, although a badly structured visual program can be much harder to comprehend than a badly formatted text. Textual programming languages also tend to be more suitable for a wider range of tasks and easier to use in terms of tooling support.

In this work we focus on a textual programming language. Our language covers a specific domain (geospatial data processing) and supports many different tasks within this domain. Modularity and extensibility is therefore of major importance for us. In this chapter we present a modelling method for a textual DSL with which we can achieve these properties. Doing something similar with a visual programming language is a task beyond the scope of this work.

[[sec:dsl-workflow-description-languages]]
=== Workflow description languages

When reviewing existing workflow description languages we have to differentiate between those that are meant to describe business workflows and those that support scientific workflows (see Chapter <<chap:processing>>). The Business Process Model and Notation (BPMN), for example, is a graphical language supporting business workflows cite:bpmn2013[]. It has elements for describing activities (tasks and subtasks), events, sequences, messages, and others. Workflows designed in BPMN typically describe a control flow in an organisation where a subsequent activity may only start when another one has finished or a certain event has happened. In this chapter we aim for designing a language that supports modelling data flow. In our language, activities consume data from a source (e.g. a Cloud data store or a previous activity) and produce new data. The control flow in our language is driven by the way the data flows from one activity to another. Apart from that, BPMN is a graphical language while we aim for creating a textual one.

WS-BPEL (Web Services Business Process Execution Language), on the other hand, is a textual language supporting the modelling of business processes that are executed by web services cite:wsbpel2007[]. It is standardised and based on XML. The current specification was defined by the Organization for the Advancement of Structured Information Standards (OASIS) and is named WS-BPEL 2.0. In contrast to BPMN, WS-BPEL also supports modelling of fully automated workflows that require no human interaction and can therefore be used to create scientific workflows. However, the language is tightly coupled with the WSDL service model cite:wsdl2001[]. Support for processing services that do not implement a WSDL interface and that are not web services is rather complex. In addition, we aim for a language that is easy to understand for domain experts. WS-BPEL is based on XML and therefore harder to read than a language that is tailored to specific domain users.

Another language for business modelling is YAWL - Yet Another Workflow Language cite:aalst2005[]. Compared to BPMN and WS-BPEL, it has stronger semantics and tries to avoid discrepancies between the workflow model and its actual implementation in the organisation. Since it is built on petri nets, it also enables formal business process validation. In addition, YAWL supports dynamic workflows that may change during the execution. Compared to the language we aim for in this work, YAWL models control flow instead of data flow. It is also based on XML and hard to read for domain users.

Apart from modelling languages for business workflows, there are a number of languages specifically supporting scientific workflows and designed for the modelling of distributed processes running in Grids or Clouds. The Common Workflow Language (CWL), for example, allows for writing workflows that are portable across different devices and target platforms cite:Amstutz2016[]. It supports data-driven workflows specifically from scientific domains including Bioinformatics, Physics, and Chemistry. CWL workflows are written in JSON or YAML. While these file formats are easier to read for humans than XML, the underlying data model is still rather complex. Users of CWL must have a knowledge about what processes they need to execute and what kind of inputs and outputs they have. Although it claims to be portable, the workflows one can describe with CWL are tightly coupled to the executed processes. The portability in CWL is achieved by running processes in isolated Docker containers. Due to these aspects, the language can better be compared to our workflow model described in Section <<sec:processing-workflow>> than to the DSL we design in this chapter.

Another technology that has gained traction recently is the Amazon States Language cite:amazon2016[]. This language is based on JSON and supports defining state machines for AWS Step Functions, a web service that facilitates orchestration of microservices in the AWS Cloud. A state machine basically models control flow and not data flow since state transitions are triggered by events and conditions--e.g. the availability of certain data, or the end of a task. The Amazon States Language requires the user to know how state machines work and how their problem can be modelled with such a machine. In addition, JSON is not as easy to read as the language we aim for in this work.

The authors of the Workflow Definition Language (WDL), on the other hand, claim that their language was specifically designed "`to be read and written by humans [...] Without requiring an advanced degree in engineering or informatics.`" cite:broadinstitute2017[]. However, WDL resembles JSON and has a rather complex data model. It also requires knowledge about the processing services to be executed. If a new service should be added to the language the user must define a '`task`' which is a description of the service's input and output parameters, but may also contain information about how the service should be executed--e.g. in a Docker container. In WDL task definitions are often intermixed with the workflow description which leads to a high coupling between workflow and the processing services. In contrast, the approach we present in this work aims for decoupling the workflow description from both the executing infrastructure and the services. In Section <<sec:interpreting-the-workflow-dsl>> we describe a rule-based way to map terms in our language to processing services. This mapping is not intermixed with the workflow script but a configuration the system developer or administrator creates. In addition, as shown in Chapter <<chap:processing>> our services are described by metadata files in JSON format which are created by the service developers and not the users writing the workflow.

=== Domain modelling methods

Domain modelling is often used in software engineering to identify major concepts in a domain and to find a suitable software design cite:evans2003[prefix=see]. A domain model consists of conceptual classes which are not necessarily software classes. Instead, domain models are typically used by software architects to communicate the conceptual structure of a system design to the domain experts as well as the developers, and to better understand the use cases and requirements in the analysed domain. The modelling method for Domain-Specific Languages we present in Section&nbsp;<<sec:modelling-method>> also makes use of domain models. They help the language designer to better understand the concepts used by domain experts and to create a language based on a vocabulary they are familiar with.

Software engineering methods can also be applied to other modelling tasks. citet:DeNicola2009[], for example, present an approach named UPON (Unified Process for ONtology building). They use this approach to build ontologies in the domains of automotive, aerospace, kanban logistics, and furniture. UPON is based on the _Unified Software Development Process_&mdash;also known as the _Unified Process_ or _UP_ cite:Jacobson1999[]&mdash;and is therefore use-case driven, iterative and incremental. Analysing use cases helps citea:DeNicola2009[] build ontologies that target a certain application area. In UPON, ontology developers work closely together with domain experts and frequently review their results in an iterative way. The ontology is then incrementally extended. citea:DeNicola2009[] conclude that their method is highly scalable and customisable. Their research shows that the use of software engineering methods outperforms existing approaches to ontology modelling.

UPON is similar to our DSL modelling approach. We also propose an iterative process that is based on best practises from software engineering. Our DSL modelling method also involves domain experts which helps us create a language that fits their needs.

In a later article, citet:DeNicola2016[] present a lightweight version of UPON called UPON Lite. The main benefits of their new approach is that it is simpler and puts domain users into focus so that they can mostly perform ontology modelling on their own without the help of ontology experts. UPON Lite encompasses six steps in which the domain users first collect a number of terms that characterise their domain before they are put into relation.

UPON Lite and our DSL modelling approach are similar to a certain degree. The differences are as follows:

* In our DSL modelling approach we also identify relevant domain terminology, but instead of asking the users to create a list of terms from scratch, we first collect user stories and then apply text analysis.

* We do not focus very much on the meaning of terms (and their relations) because we are more interested in the terms themselves instead of their semantics.

* The outcome of our approach is not an ontology but a grammar for a Domain-Specific Language.

Applying methods from software engineering to both ontology modelling and DSL modelling is novel. citea:DeNicola2009[] show that these methods are suitable for building ontologies. In the following, we do so for Domain-Specific Languages.

[[sec:modelling-method]]
== DSL modelling method

In order to design the Domain-Specific Language for distributed geospatial data processing, we propose a novel incremental and iterative modelling method consisting of the following steps:

. Analyse the application domain
. Create user stories
. Analyse user stories and look for relevant subjects, objects, adjectives, and verbs
. Create a domain model using subjects and objects found in the user stories as classes
. Identify relevant verbs which become actions in the Domain-Specific Language
. Build sample DSL scripts based on the modelled domain
. Derive formalized grammar from the sample DSL scripts
. Review and reiterate (go back to step 1) if needed

// [#img-dsl-modelling-method.top]
// .Incremental and iterative modelling method to create a Domain-Specific Language
// image::images/06_dsl_modelling/method.pdf[width="35%",align="center"]

The first couple of steps are inspired by object-oriented software engineering. Steps 1 and 2 are essential for every modern agile and lean software development project. They belong to the general requirements analysis phase where software developers and stakeholders (most likely domain users) work together to identify functional and non-functional requirements. This typically results in a number of user stories describing how the domain users would like to interact with the system. Ideally, these stories are created by the users themselves. This ensures they are written in "`the user's own words`", which basically means they use the exact same vocabulary the domain users are used to from their everyday work. Based on this, a Domain-Specific Language can be created that contains terms from the application domain and that is hence easy to understand and learn.

The text analysis performed in step 3 of the DSL modelling process provides the basis for the domain vocabulary. The collected objects, subjects and verbs have to be structured and classified, so a machine-readable programming language can be defined. In step 4, a domain model is created describing the relations between subjects and objects. Additionally, in step 5 the relevant verbs are identified that will later become actions in the Domain-Specific Language.

In step 6 sample scripts written in the (not yet formalised) DSL are created. Just like in the first two steps, the domain users should be involved here to provide feedback on the sample scripts. This ensures the final language will look as expected. After that, a formalised grammar is created in step 7. This makes the language machine-readable and interpretable.

The whole modelling process is iterative. The result is reviewed and revised if necessary in close collaboration with the domain users. In the following we demonstrate our DSL modelling method by applying it to two example use cases. The use cases are described in detail in Section&nbsp;<<sec:introduction-use-cases>>. We only repeat relevant parts here.

[[sec:dsl-example-use-case]]
== Use case A: Urban planning

This section focuses on the use case described in Section <<sec:introduction-use-case-a>> dealing with urban planning. We demonstrate how a Domain-Specific Language tailored to experts from this domain can be created. The use case includes tasks from the typical work of an urban planner who needs to integrate and process geospatial data from different sources to create products such as topographic maps, orthophotos, and 3D city models.

The _first step_ in our DSL modelling method is the domain analysis. As a result of the domain analysis, we create a number of user stories (_step 2_). We refer to Section <<sec:introduction-use-case-a>> where we have already described and analysed the use case in detail. For reasons of clarity and comprehensibility, we repeat the user stories here:

[quote]
____
**User story A.1:** As an urban planner, I want to capture topographic objects (such as cable networks, street edges, urban furniture, traffic lights, etc.) from data acquired by mobile mapping systems (LiDAR point clouds and images) so I can create or update topographic city maps.
____

[quote]
____
**User story A.2:** As an urban planner, I want to automatically detect individual trees from a LiDAR point cloud in an urban area, so I can monitor growth and foresee pruning work.
____

[quote]
____
**User story A.3:** As an urban planner, I would like to update my existing 3D city model based on analysing recent LiDAR point clouds.
____

[quote]
____
**User story A.4:** As an urban planner, I want to provide architects and other urban planners online access to the 3D city model using a
simple lightweight web client embedded in any kind of web browser, so that they are able to integrate their projects into the model and share it with decision makers and citizens for communication and project assessment purposes.
____

Specific focus is put on the process of updating a 3D city model based on LiDAR point clouds (user story A.3). In order to complete this task, the urban planner typically performs the following operations:

i)   Remove non-static objects (such as cars, rubbish bins, bikes, or people) from the point cloud.
ii)  Characterize changes of static objects (such as trees, bus stops, or façade elements).
iii) Include new or exclude existing classified objects (e.g. roof tops or antennas).
iv)  Use the result to update the city model (i.e. apply the changes to the existing model).

The urban planner typically visualises the results in 3D to assess correctness and overall quality. Such a 3D visualisation can be presented to decision makers, for example, if changes in the city such as new constructions should be discussed. This is reflected in user story A.4 and also described in our previous work cite:dambruch-kraemer-2014[]. We do not analyse A.4 in detail in the following sections because it contains many technical elements. We only take into account that the users wish to visualise the processing results at the end.

=== Vocabulary/Taxonomy

According to _step 3_ of the DSL modelling method, the user stories from the previous section now have to be analysed to find subjects and objects that can later be used as classes in the domain model. Verbs and adjectives are also important. They will become actions and parameters in the Domain-Specific Language in the end.

Tables <<tab:use-case-a-results-text-analysis-objects>>, <<tab:use-case-a-results-text-analysis-verbs>> and <<tab:use-case-a-results-text-analysis-adjectives>> depict the results of this analysis with regard to identified subjects/objects, verbs and adjectives. Please note that not all terms found in the user stories are actually relevant. For example, the verbs '`monitor`' and '`foresee`' as well as the objects '`growth`' and '`pruning work`' refer to something that happens _after_ the urban planner has processed the data. They do not belong to the processing itself and hence do not appear in the Domain-Specific Language.

Also note, at certain points, the wording in the user stories is unclear. For example, the expression '`changes of static objects`' is rather unspecific about what '`changes`' actually are. The only possibility to resolve this issue is to ask the users. In our case, they said that for them '`changes`' mean that objects are added to a dataset or removed from it. We therefore put the adjectives '`added`' and '`removed`' in the table.

We tried to identify the singular form of all subjects and objects. This helped us later to make the domain model more consistent. In the case of '`people`' we chose '`person`'.

[[tab:use-case-a-results-text-analysis-objects]]
.Subjects and objects identified in the text analysis
[cols="3*^",frame=none,grid=all]
|===

| topographic object
| cable network
| street edge

| urban furniture
| traffic light
| image

| LiDAR point cloud
| mobile mapping system
| topographic city map

| tree
| urban area
| growth

| pruning work
| 3D city model
| object

| car
| rubbish bin
| bike

| person
| bus stop
| façade element

| roof
| antenna
|

|===

[[tab:use-case-a-results-text-analysis-verbs]]
.Verbs identified in the text analysis
[cols="3*^",frame=none,grid=all]
|===

| capture
| create
| update

| detect
| monitor
| foresee

| remove
| characterize
| include

| exclude
| visualise
|

|===

[[tab:use-case-a-results-text-analysis-adjectives]]
.Adjectives identified in the text analysis
[cols="3*^",frame=none,grid=all]
|===

| non-static
| static
| recent

| added
| removed
|

|===

[[sec:use-case-a-domain-model]]
=== Domain model

The next step in the modelling process is to create a domain model based on the text analysis and the subjects and objects found. Figure <<img-use-case-a-domain-model>> shows the domain model for this example use case. `CityModel`, `PointCloud`, and `Image` are datasets containing topographic objects. Each `TopographicObject` is either a `StaticObject` (`Roof`, `Antenna`, `Facade`, `Tree`, etc.) or a `NonStaticObject` (`People`, `Bike`, `Car`, etc.).

The domain model helps to structure the heap of terms found in the text analysis and to differentiate between relevant and irrelevant words.

[#img-use-case-a-domain-model]
.Domain model of the example use case A. For the sake of readability, some classes (datasets and static objects) have been left off.
image::images/06_dsl_modelling/use-case-a-domain-model.pdf[scaledwidth="100%",align="center"]

[[sec:use-case-a-relevant-verbs]]
=== Relevant verbs

In _step 5_ of our modelling method we revise the list of verbs identified earlier and select those that should become actions in the Domain-Specific Language. We can split the verbs into two categories: one containing terms that relate to actions that the urban planner does (e.g. '`capture`', '`monitor`' and '`foresee`') and another for commands the system should execute (e.g. '`remove`', '`exclude`' and '`visualise`'). Applying this to all verbs from Table <<tab:use-case-a-results-text-analysis-verbs>> results in the list of actions shown in Table <<tab:use-case-a-results-verbs-to-actions>>.

[[tab:use-case-a-results-verbs-to-actions]]
.Verbs that could become actions in the Domain-Specific Language
[cols="4*^",frame=none,grid=all]
|===

| detect
| remove
| characterize
| include

| exclude
| visualise
| update
|

|===

This list can further be simplified by merging similar terms. '`detect`', '`characterise`' and '`include`' can be represented by the more generic verb '`select`'. The final list is shown in table <<tab:use-case-a-results-actions>>

[[tab:use-case-a-results-actions]]
.Final list of actions in the Domain-Specific Language
[cols="4*^",frame=none,grid=all]
|===

| select
| exclude
| update
| visualise

|===

[[sec:sample-dsl]]
=== Sample DSL script

In _step 6_ of our DSL modelling method we create sample scripts to be able to derive a generic grammar for the DSL.

In order to update the 3D city model and visualise the results (user story A.3), the following sample is proposed.

[source,subs=+quotes]
----
*with* recent PointCloud *do*
    *exclude* NonStaticObjects
    *select* added Trees *and* added FacadeElements
    *update* CityModel
*end*

*with* CityModel *do*
    *exclude* Antennas
    *visualize*
*end*
----

The script consists of two parts. In the first one a recently acquired point cloud dataset is processed. Non-static objects are removed and new trees and façade elements are detected. These new objects are added to the city model. In the next block all antennas are removed from the city model and the result is visualised on the screen.

The script is easy to read and shows exactly what processing steps are performed. The Domain-Specific Language proposed here makes use of terms from the domain model and from the results of the text analysis performed before. Domain users can therefore quickly learn the language and use it for their own needs.

Similar sample scripts can be created for the other two user stories A.1 and A.2.

[[sec:use-case-a-dsl-grammar]]
=== DSL grammar

In order to make the Domain-Specific Language machine-readable, its grammar and syntax have to be formalised. A common way to do this is the specification of either a context-free grammar (CFG) using EBNF (Extended Backus–Naur Form). Alternatively, a PEG (Parsing Expression Grammar) can be used. One of the benefits of PEGs is that they can never be ambiguous. They are therefore very easy to define and are often not as complex as CFGs, not least because they do not require an additional tokenisation step. On the other hand, PEGs require more memory than CFGs, but for small languages such as DSLs this disadvantage can be neglected. The PEG for the sample DSL script presented in the previous section is as follows:

....
start = SP* statements SP*

statements = statement ( SP+ statement )*

statement = block / operation

block = with SP+ statements SP+ "end"

with = "with" SP+ dataset SP+ "do"

dataset = "recent" SP+ ID / ID

operation = "visualize" / "exclude" SP+ ID /
    "select" SP+ param SP+ ID ( SP+ "and" SP+ param SP+ ID )* /
    "update" SP+ dataset

param = "added"

ID = [_a-zA-Z][_a-zA-Z0-9]*

SP = [ \t\n\r]
....

The syntax used to specify the PEG here is the one of the tool PEG.js, an open-source parser generator written in JavaScript cite:pegjs2017[]. Square brackets are used to define regular expressions. The plus character '`+`' means one or more occurrences whereas the asterisk '`*`' means zero or more occurrences. The slash character '`/`' is used to specify alternatives.

Note that a grammar for the complete example use case would be much larger. The PEG shown here can only be used to parse the example script from the previous section. For the sake of readability, additional grammar rules have been left off. We will create a complete grammar covering both example use cases in Section <<sec:dsl-use-case-b>>.

=== Reiteration

The final step of the modelling process is to review the Domain-Specific Language and to revise it if necessary. The language presented in this chapter already is a result of several iterations in which we worked together with domain users to create to a reasonable and sensible DSL that meets the users`' expectations.

=== Rationale for the chosen syntax

The sample DSL presented before is based on the vocabulary from the domain model. Getting to the specific syntax was a matter of testing various alternatives and evaluating how they work in certain use cases. The following sample script was used as a starting point.

[subs=+quotes]
....
*with* PointCloud
*exclude* NonStaticObjects *from* it
*select* added Trees *and* added FacadeElements *from* it
*add* it *to* CityModel
....

In this sample script, the keyword `it` is used to refer to an object from the previous line or to refer to the result of the process performed in the previous line. This context-sensitive approach requires an intelligent parser that is able to clearly identify what `it` means in the respective context. While working with the domain users we noticed they had problems understanding such context-sensitive scripts. Instead, the following syntax was tested.

[source,subs=+quotes]
----
**exclude** NonStaticObjects *from* PointCloud
*and* *select* added Trees *and* added FacadeElements
*and* *add to* CityModel
----

In this case, individual processing steps are connected with the `and` keyword. While this approach leads to unambiguous scripts, they still can quickly become hardly readable. The longer the scripts get, the harder it is to understand them as the sentences become longer and longer. On the other hand, blocks enclosed by `with` ... `do` and `end` (like they are used in Section <<sec:sample-dsl>>) make clear which processing steps affect which data set.

[[sec:dsl-use-case-b]]
== Use case B: Land monitoring

In the last section we applied our DSL modelling method to the urban planning use case. We now do the same for the other use case dealing with land monitoring (see Section <<sec:introduction-use-case-b>>). In doing this, we try to align the DSL grammar with the one from Section <<sec:use-case-a-dsl-grammar>>. This allows us to create a Domain-Specific Language that is flexible enough to cover both use cases.

Again, we start with analysing the user stories.

[quote]
____
**User story B.1:** As an hydrologist or a geo-morphologist supporting decision makers in civil protection, I want to analyse data measured during critical events to prepare better prediction and monitoring of floods and landslides.
____

[quote]
____
**User story B.2:** As an hydrologist, I want to study the evolution of measured precipitation data as well as slope deformation from optical images, compute parameters to produce high-quality input for hydrological and mechanical modelling and simulation, and compare the results to reference measurements obtained for flooding events and landslides.
____

User story B.1 describes the overall goal of this use case. B.2, on the other hand, contains three individual steps to achieve this goal:

. Study the evolution of precipitation data as well as slope deformation.
. Compute parameters to produce high-quality input for hydrological and mechanical modelling and simulation.
. Compare the results to reference measurements.

The first step is something that the user does outside the system. It can be ignored. Step 2, however, is rather complex and actually consists of the following partial steps:

[lowerroman]
. Resample the available terrain data (point cloud) to match a given density.
. Remove outliers from the point cloud.
. Split the point cloud into areas defined by drainage boundaries.
. Reorder points according to their "`relevance`"&mdash;i.e. how much they contribute to the appearance of the terrain--and store them together with the drainage basins hierarchy.

For the final step 3 the following additional partial steps have to be performed:

[lowerroman,start=5]
. Extract a single level of detail (i.e. a single resolution) from the reordered points.
. Perform a constrained triangulation to create a mesh from the point cloud preserving constraints such as boundaries and feature lines.

=== Vocabulary/Taxonomy

Similar to the previous use case, we now analyse the user stories and extract relevant subjects/objects, verbs and adjectives. The results are shown in Tables <<tab:use-case-b-results-text-analysis-objects>>, <<tab:use-case-b-results-text-analysis-verbs>>, and <<tab:use-case-b-results-text-analysis-adjectives>>.

Again, we list the singular form of all subjects and objects. We identified the base form of verbal nouns such as '`prediction`' (to '`predict`') and '`monitoring`' (to '`monitor`'), but also '`modelling`' (to '`model`') and '`simulation`' (to '`simulate`'), and put them in the list of verbs instead of objects.

[[tab:use-case-b-results-text-analysis-objects]]
.Subjects and objects identified in the text analysis
[cols="3*^",frame=none,grid=all]
|===

| event
| flood
| landslide

| evolution
| precipitation data
| slope deformation

| optical image
| parameter
| input

| reference measurement
| outlier
| terrain data

| point cloud
| density
| area

| drainage boundary
| relevance
| terrain appearance

| drainage basin
| hierarchy
| level of detail

| resolution
| triangulation
| mesh

| constraint
| feature line
|

|===

[[tab:use-case-b-results-text-analysis-verbs]]
.Verbs identified in the text analysis
[cols="3*^",frame=none,grid=all]
|===

| analyse
| prepare
| predict

| monitor
| study
| compute

| model
| simulate
| compare

| remove
| resample
| split

| reorder
| store
| extract

| perform
| create
|

|===

[[tab:use-case-b-results-text-analysis-adjectives]]
.Adjectives identified in the text analysis
[cols="3*^",frame=none,grid=all]
|===

| critical
| high-quality
| hydrological

| mechanical
| constrained
|

|===

[[sec:use-case-b-domain-model]]
=== Domain model

According to our DSL modelling method, we now build the domain model based on the results from the previous section.

[#img-use-case-b-domain-model]
.Domain model of the example use case B
image::images/06_dsl_modelling/use-case-b-domain-model.pdf[scaledwidth="100%",align="center"]

We make the following assumptions:

* We do not differentiate between input and output. Similar to the domain model from Section&nbsp;<<sec:use-case-a-domain-model>> we use the term '`dataset`' instead.
* We do not include '`terrain appearance`' in the diagram. It just appears in a sub-clause to describe the term '`relevance`'.
* The terms '`area`', '`feature line`' and '`drainage boundary`' refer to geometries. We therefore add the term '`geometry`' and reduce '`drainage boundary`' to '`boundary`'.
* A '`mesh`' is defined by a '`triangulation`'. Both are geometries.
* In the context of the use case a '`single resolution`' is a '`level of detail`'. We only include the latter.
* In Section <<sec:use-case-a-domain-model>> we merged objects and adjectives. Here, this is only necessary for '`constrained`' and '`triangulation`'. The other adjectives are either rating ('`critical`' and '`high-quality`') or refer to verbal nouns ('`hydrological`' and '`mechanical`').

=== Relevant verbs

Similar to the first use case, we now identify verbs that should become actions in the Domain-Specific Language. Again, we remove verbs that refer to activities the user performs outside the system (e.g. '`analyse`' or '`prepare`'). The verbs '`compute`', '`perform`' and '`create`' describe similar activities. We therefore only include '`create`'. The result is shown in table <<tab:use-case-b-results-verbs-to-actions>>.

[[tab:use-case-b-results-verbs-to-actions]]
.Verbs that could become actions in the Domain-Specific Language
[cols="4*^",frame=none,grid=all]
|===

| create
| remove
| resample
| split

| reorder
| store
| extract
|

|===


=== Sample DSL script

According to our DSL modelling method (step 6) we now create sample scripts. We try to reuse as much of the grammar from the previous use case as possible and propose the following script covering the first four sub-steps from user story B.2:

[source,subs=+quotes]
----
*with* PointCloud *do*
    *remove* Outliers
    *resample* *using* density: 10
    *split* *with* DrainageBoundaries *as* areas

    *for each* areas *do*
        *reorder* *using* method: "relevance"
        *store*
    *end*
*end*
----

The script operates on a point cloud. It first removes all outliers and resamples the dataset according to a given density. It then splits the resampled point cloud along given drainage boundaries. The result is a list of smaller point clouds named '`areas`'. The script iterates over all areas and reorders the points according to their relevance. It stores each result to the Cloud.

The final two sub-steps from user story B.2 can be covered by the following script:

[source,subs=+quotes]
----
*with* Area *do*
    *extract* LevelOfDetail *using* lod: 10
    *create* ConstrainedTriangulation
*end*
----

Compared to the DSL from use case A this sample script introduces the following new keywords and language constructs:

* The keyword '`using`' can be used to specify a value of a named parameter in the form '`name: value`'. Parameter values can be numbers or strings (in double quotes).
* In the first use case we used the keyword '`with`' to apply a number of operations to a dataset. We put these operations in a '`do ... end`' block. The new sample DSL script enables using '`with`' in a single operation too.
* The keyword '`as`' can be used to name the result of an operation.
* The new construct '`for each`' allows for applying a number of operations to a list. The operations are specified in a '`do ... end`' block.

Creating the domain model in Section <<sec:use-case-b-domain-model>> was particularly useful in this case because it helped us differentiate between datasets (which are specified by '`with <dataset>`') and parameters (specified by '`using <name>: <value>`').

With the new keywords we can generalise the grammar from the previous use case. We can replace '`select`' by '`extract`' and make use of '`using`' to specify what type of objects we want to extract. The sample script from Section <<sec:sample-dsl>> now looks as follows:

[source,subs=+quotes]
----
*with* recent PointCloud *do*
    *exclude* NonStaticObjects
    *extract* StaticObjects *using* type: "Trees" *and* type: "FacadeElements"
    *update* CityModel
*end*

*with* CityModel *do*
    *exclude* Antennas
    *visualize*
*end*
----

[[sec:use-case-b-dsl-grammar]]
=== Generic DSL grammar and properties

The final step of our DSL modelling process is reiteration. As mentioned before, the language presented in this chapter already is a result of several iterations. In these iterations we were able to identify a couple of generic language properties as well as additional elements. We also generalised elements such as the '`for`' expression or the way in which operations can be applied.

The following list is an overview of the language properties as well as the additional and the generalised language elements.

[role="mt-1"]
**Execution order.** The DSL presented here is a declarative and functional language. The order in which statements are executed is not strictly sequential. In Chapter <<chap:processing>> we have shown that our system executes processing services according to a dependency graph and may run two or more processing services in parallel if possible and beneficial. The DSL presented here supports modelling of such a graph. Individual statements can depend on the results of their respective prior statement and with the use of names (see below) even any prior statement. Even though statements appear one after the other it does not mean they will be executed sequentially. In fact, the following three scripts mean the same thing:

[source,subs=+quotes]
----
*remove* Outliers *with* [PointCloudA] *as* cleanPointCloud
*resample* *with* [PointCloudB] *using* density: 10
*split* *with* cleanPointCloud *and* DrainageBoundaries *as* areas
----

[source,subs=+quotes]
----
*resample* *with* [PointCloudB] *using* density: 10
*remove* Outliers *with* [PointCloudA] *as* cleanPointCloud
*split* *with* cleanPointCloud *and* DrainageBoundaries *as* areas
----

[source,subs=+quotes]
----
*remove* Outliers *with* [PointCloudA] *as* cleanPointCloud
*split* *with* cleanPointCloud *and* DrainageBoundaries *as* areas
*resample* *with* [PointCloudB] *using* density: 10
----

[role="mt-1"]
**Names.** Our language offers a way to name the result of an operation using the '`as`' keyword. Names can only be assigned once. Their meaning must not change during the course of a workflow. This means all names are actually constants or immutable variables. This allows us to avoid side-effects that would make workflow scheduling in a distributed environment too complex. The interpreter presented in Section <<sec:interpreting-the-workflow-dsl>> performs semantic validation to prevent that a name is assigned more than once.

[source,subs=+quotes]
----
*remove* Outliers *with* PointCloud *as* newPointCloud
*split* *with* DrainageBoundaries *as* newPointCloud    // <- error!
----

[role="mt-1"]
**For expression.** In contrast to other (general-purpose) languages our '`for`' expression is not a loop. There is no counter and no termination condition. The iterations are not necessarily executed sequentially. In fact, our '`for`' expression can be better compared to a functional higher-order '`map`' in which a set is mapped to another one by applying a given function _f_ to all elements. Since our language does not allow for side effects, _f_ can be applied to multiple elements in the set in parallel without causing conflicts.

[role="mt-1"]
**Yield.** A '`for`' expression maps a set to a new set. Consequently, our language offers a way to make one '`for`' expression depend on the results of another. We introduce the '`yield`' keyword allowing users to explicitly declare the elements of which the new set should consist. This keyword is optional. If it is left off, the results of the last operation inside the '`for`' expression will be collected into the new set. The following scripts therefore mean the same:

[source,subs=+quotes]
----
*for each* PointCloud *do*
    *reorder* *using* method: "relevance" *as* reorderedPointCloud
    *yield* reorderedPointCloud
*end as* setOfReorderedPointClouds

*for each* setOfReorderedPointClouds *do*
    *resample* *using* density: 10
*end*

// means the same as:

*for each* PointCloud *do*
    *reorder* *using* method: "relevance"
*end as* setOfReorderedPointClouds

*for each* setOfReorderedPointClouds *do*
    *resample* *using* density: 10
*end*
----

Note that in order to be able to iterate over the result of a '`for`' expression we have to give it a name (in this case '`setOfReorderedPointClouds`').

[role="mt-1"]
**Generic '`apply`'.** In Section <<sec:interpreting-the-workflow-dsl>> we will show that our interpreter maps language terms to processing services. This allows for a high-level workflow description without requiring knowledge of the underlying execution system and the processing services that are actually applied. However, during the reiteration phase, some domain users requested to have more control over the processing services and their parameters. Those users were more familiar with the actual algorithms available and wanted to apply them directly. We therefore introduced the keyword '`apply`'. The following script applies a processing service called '`OutlierRemoval`' to a point cloud:

[source,subs=+quotes]
----
*apply* OutlierClassificationInPointCloud *with* [PointCloud]
    *using* outlierFilteringK: 15 *and* outlierFilteringStddev: 3.0
----

This script means the same thing as the high-level expression we used in the previous examples where we relied on default values for '`outlierFilteringK`' and '`outlierFilteringStddev`':

[source,subs=+quotes]
----
*remove* Outliers *with* PointCloud
----

[role="mt-1"]
**Placeholders.** Similar to the generic '`apply`' keyword, domain users requested to have more control over which dataset they apply the processing workflow to. They also wanted to be able to reuse scripts and apply them to multiple datasets. We therefore introduced means to specify placeholders in the workflow script. They represent a dataset and can be replaced (or filled in) by the user interface calling the workflow interpreter (see Section <<sec:interpreting-the-workflow-dsl>>). Placeholders are specified in square brackets:

[source,subs=+quotes]
----
*with* [PointCloud] *do*
    ...
*end*
----

Appendix <<app:combined-dsl-grammar>> shows the final PEG which comprises the grammar from Section <<sec:use-case-a-dsl-grammar>>, the elements from applying the modelling method to use case B, as well as the generalised language elements.

[[sec:interpreting-the-workflow-dsl]]
== Interpreting the workflow DSL

In this section we describe how the Domain-Specific Language designed in this chapter can be interpreted and how the individual language elements can be mapped to processing services. In order to meet the quality attributes defined in Section <<sec:architecture-quality-attributes>>, in particular modifiability, we propose a modular approach that enables us to change both the language and the processing services later on independently without affecting the other.

To this end, we implement a component called _Interpreter_ that translates a workflow script to a machine-readable workflow model as described in Section <<sec:processing-workflow>>. The interpreter first parses a workflow script into an abstract syntax tree (AST). It then traverses the AST, performs semantic checks and produces executable workflow actions for each visited node. The process is shown in Figure <<img-interpreter>> and described in detail in the following sections.

[#img-interpreter.top]
.A workflow script is parsed to an Abstract Syntax Tree (AST) and then mapped to a machine-readable workflow model
image::images/06_dsl_modelling/interpreter.pdf[scaledwidth="78%",align="center"]

=== Parsing and traversing

The DSL grammar shown in appendix <<app:combined-dsl-grammar>> can be used to generate a parser with the open-source tool PEG.js. This parser produces an Abstract Syntax Tree (AST) that represents the individual tokens in the workflow script.

For example, consider the following workflow:

[source,subs=+quotes]
----
*for each* [PointCloud] *do*
    *resample* *using* density: 10
*end*
----

This workflow is parsed to the AST depicted in Figure <<img-example-ast>>. The root node in the AST represents the whole workflow. The other nodes denote the individual expressions and language elements. Each node has a number of properties that are either literals or link to other nodes (sub-expressions).

The AST is traversed twice using DFS (Depth-First Search). The first time the interpreter performs semantic checks and builds a symbol table (see Section <<sec:dsl-semantic-analysis>>). The second time it translates the AST nodes into executable workflow actions (see Section <<sec:dsl-code-generation>>).

[#img-example-ast.top]
.An example Abstract Syntax Tree
image::images/06_dsl_modelling/AST.pdf[scaledwidth="55%",align="center"]

[[sec:dsl-semantic-analysis]]
=== Semantic analysis

In the semantic analysis phase the interpreter traverses the AST and checks the semantics of the workflow script. It makes use of type signatures defining inputs and outputs of the individual operations. The set of signatures is mainly derived from the service metadata described in Section&nbsp;<<sec:processing-service-metadata>> but may also contain custom elements for language constructs that cannot be mapped one-to-one to a processing service.

The interpreter uses the type information to perform the following validations:

* A statement must be mappable to one or more processing services (see Section <<sec:dsl-code-generation>>).
* A statement must contain values for all mandatory parameters of the processing services it maps to. A missing value may be filled in from the implicit context--either by using the dataset from an enclosing '`with`' or '`for`' block, or the result from the previous statement.
* A statement must not contain additional parameters not defined in the type signatures.
* The types of all parameters must be correct (according to the type signatures).
* Names must be declared before use.
* Names must not be assigned more than once.

During semantic analysis the interpreter also creates symbol tables. This data structure keeps information about names such as their type or the location of declaration. The interpreter uses these symbol tables to check for the existence of names and to prevent duplicate name assignments.

The symbol tables are kept in a stack. At the beginning, the top of the stack is the global symbol table (the one that contains declarations made on the top level of the script). Whenever the interpreter visits a block node ('`with`' or '`for`') in the AST, it puts a new symbol table on top of the stack and removes it again when it has completely visited the node and all its children. In order to look up a name in the stack--i.e. to obtain its type or to check if it has been declared before--the interpreter starts with the symbol table on the top and then continues to the bottom until it either finds the name or reaches the end of the stack.

[[sec:dsl-code-generation]]
=== Code generation

In order to create a workflow structure that can be executed by the JobManager (see Chapter <<chap:processing>>), the interpreter traverses the AST a second time. It makes use of a set of pre-defined rules that map AST nodes to processing services, service parameters and datasets. To illustrate the different types of mappings, Figure <<img-dsl-mapping-ast-to-awf>> shows examples that appear in use case A.

[#img-dsl-mapping-ast-to-awf]
.Nodes in the Abstract Syntax Tree are mapped to data sets and processing services
image::images/06_dsl_modelling/mappings.pdf[scaledwidth="58%",align="center"]

* **One-to-one mapping.** If a term such as '`exclude`' appears in the AST, the interpreter maps it to exactly one processing service--in this case a filter removing objects that should be excluded.
* **Many-to-one mapping.** The terms '`recent`' and '`CityModel`', for example, are mapped to a specific dataset (or file) which represents the latest version of the 3D city model kept in the distributed Cloud storage.
* **One-to-many mapping.** Terms such as '`Trees`' may need to be mapped to parametrised processing services. For example, the processing service for feature extraction is implemented using machine learning algorithms. The term '`Trees`' therefore needs to be mapped to both the feature extraction service and a pre-trained classifier for trees.

In addition, many-to-many relations can also happen although they do not appear in the example use cases.

The mapping rules are stored in a configuration file that can either be written in JSON or YAML. This allows the mapping to be modified without requiring the developer to recompile the interpreter.

An initial configuration file can be generated from the service metadata described in Section&nbsp;<<sec:processing-service-metadata>> by applying a one-to-one mapping for the processing services and all their parameters. This configuration can then be modified to customise the Domain-Specific Language.

By the use of mapping rules as it is proposed here, the code generator may be replaced without affecting the DSL, the parser, or the semantic analyser. This means that even if the back-end (i.e. the processing services or the mapping rules) are replaced or modified, the scripts written by the domain users stay the same. In particular, this ensures that domain knowledge the users put into the scripts remains valid for a long time even if the underlying Cloud infrastructure changes--e.g. if the infrastructure is transferred from one Cloud provider to another.

In addition, the mapping rules allow for creating a modular Domain-Specific Language that consists of a basic set of keywords as well as a number of elements that are dynamically mapped to processing services, parameters and datasets. The Domain-Specific Language we describe here is therefore not cast in stone but can be adapted to further use cases.

Similar to the semantic analysis phase the interpreter keeps a data structure in the code generation phase containing information about names and their current values. This data structure is called _environment_. The interpreter maintains a stack of environments analogous to the stack of symbol tables in the semantic analysis. Each environment contains key-value pairs for all names valid in the current context (i.e. scope). This data structure is necessary to infer parameters from the context, so the statements are correctly converted to linked actions in the machine-readable workflow model.

[[sec:dsl-user-interface]]
== User interface

As described in Chapter <<chap:architecture>>, our system contains an editor allowing users to define their processing workflows using our Domain-Specific Language. To create a prototype of such a workflow editor, we implemented a web-based application as shown in Figure <<img-workflow-editor-screenshot>>.

[#img-workflow-editor-screenshot.bottom]
.Screenshot of our web-based workflow editor
image::images/06_dsl_modelling/workflow-editor-screenshot.png[scaledwidth="97%",align="center"]

We designed the user interface according to the following principles: reuse existing standards and behaviours, and keep navigational elements at the necessary minimum. We evaluated some of the most popular Integrated Development Environments (IDEs) and editors such as Eclipse, IntelliJ IDEA, Visual Studio Code, and Sublime Text and selected elements relevant to our workflow editor. Our application consists of the following parts:

* The main area on the right where users can enter a workflow using the Domain-Specific Language
* The menu bar on the top with items to create a new workflow, save the current one, undo, redo, etc.
* A sidebar on the left containing
** The list of all created workflows
** A cue card showing hints while the user is typing

The editor allows users to design completely new workflows or to load and edit existing ones. Workflows are stored in a database. Users can decide whether they want to keep created workflows private or if they want to make them available to other users in the system.

The editor's main area offers a couple of useful features that support users in editing a workflow:

* _Syntax highlighting_ helps identify the main elements of the DSL. The editor colourises individual tokens in the workflow according to their syntactical meaning. For example, keywords are displayed in red, numbers in green, comments in grey, and placeholders in blue.
* The _auto-completion feature_ displays a list of suggestions while the user is typing. For example, if the user types a '`c`' the editor will display a list of all keywords and processing services starting with this character. This feature speeds up the workflow definition and helps beginners to learn the language faster.
* _Error reporting_ highlights issues in the workflow. The editor automatically compiles and validates the code in the background. Invalid statements (e.g. typos, missing processing services, invalid parameters) are underlined in red and an icon is displayed next to the number of the line containing the invalid statement. The user can hover the mouse over the icon to get a tooltip with the complete error message.

The sidebar on the left either displays a list of all workflows stored in the database and accessible to the user, or a cue card showing hints while the user is typing. The cue card's contents change dynamically depending on where the user has placed the cursor in the editor. For example, if the cursor is on a keyword the cue card will show a description of this keyword. If the user has typed an '`a`' the cue card will display all keywords and processing services starting with this character including their description. If the user has selected the name of a processing service, the cue card will display the service parameters including descriptions and information about whether these parameters are mandatory or optional as well as their default values. Figure <<img-workflow-editor-cue-cards>> depicts two example cue cards--one showing a processing service named '`Dimensionality`' and its parameters, and another one showing keywords the user may insert at the current cursor position including a short description.

We implemented the workflow editor using AngularJS cite:angularjs2017[]. This framework supports the MVC (Model-View-Controller) pattern and allows for modularising the web application so that new components can be added later without affecting existing ones.

[#img-workflow-editor-cue-cards.top]
.Two cue cards showing information about a processing service named '`Dimensionality`' (left), and a list of keywords (including their description) which the user could enter at the current cursor position (right)
image::images/06_dsl_modelling/cue-card-dimensionality.png[scaledwidth="85%",align="center"]

== Summary

In this chapter we described a user interface for the processing of large geospatial data in the Cloud. The interface is facilitated by a Domain-Specific Language. In order to create a language that uses domain vocabulary, we introduced a novel method for DSL modelling. This method makes the language easy to understand and learn, and allows domain users to write their own processing scripts without a deep knowledge of the underlying Cloud infrastructure. In particular, the users do not have to care about on what specific hardware the processing is executed or what algorithms are exactly used and with what parameters they are called. The Domain-Specific Language offers just as much control as the users need. The rest is hidden in mapping rules that control how the language interpreter translates terms in a script to actual calls of processing services in the Cloud.

This chapter specifically focused on two use cases related to urban planning and land monitoring, but both the DSL modelling method and the approach to map language terms to processing services are independent of any application domain and can be applied to other areas as well. Due to the configurable mapping approach, our language is very modular. It can easily be extended by registering new processing services and amending the mapping rules.

The designed language contains generic constructs such as the '`apply`' keyword which allows users to control exactly what services should be executed with which parameters, in contrast to the high-level language constructs which provide a better usability by hiding details. Generic keywords and high-level domain-specific constructs can be intermixed in the same workflow. This allows domain users to select the right balance between usability and flexibility, depending on their personal experience.

Basically, the approach presented in this chapter is intended to close the gap between the processing of geospatial data in the Cloud and the end users who are GIS experts, but typically not experts in Computer Science, and in particular not in Cloud Computing or Big Data. In the future, Cloud technology will be used more and more often in the geospatial domain. Previous work has shown that this development has many benefits for all stakeholders cite:Khan2013_1,kraemer-ludlow-khan-2013[], but nonetheless a user interface that is easy to understand (like one that is based on a Domain-Specific Language) may be key to its success.
