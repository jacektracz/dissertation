[[chap:evaluation]]
= Evaluation

In this chapter we present the results from performing a qualitative and quantitative evaluation of our architecture and our implementation respectively. We show that our system meets the stakeholder requirements and the quality attributes specified in Chapter <<chap:architecture>>. To this end, we first define specific evaluation scenarios and apply them to real-world data sets from our use cases introduced in Section <<sec:introduction-use-cases>>. This enables us to validate whether the system actually has the specified quality attributes or not. After this, we discuss stakeholder requirements and how our system satisfies them. We also validate that the overall objectives of our thesis and the general requirements from our two user groups from the problem statement are met. The chapter concludes with a summary of the evaluation results.

[[sec:evaluation-environment]]
== Environment

In order to perform the quantitative evaluation, we deployed our system to a Cloud environment. At the Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany we had access to an OpenStack Cloud cite:openstack2017[] with 3 physical controller nodes, 13 compute nodes, and 6 storage nodes with the following specifications:

[cols="15,85a",frame=none,grid=none]
|===
2+|_Controller and compute nodes:_
| *CPU:*
| 2 &times; Intel&reg; Xeon&reg; E5-2660v4 2.0GHz 14 Core LGA 2011
| *RAM:*
| 8 &times; DIMM DDR4-2400 32GB ECC REG
| *HDD:*
| 2 &times; Samsung SSD SM863 480GB
| *Network:*
| 4 &times; 10GBit/s Ethernet
2+|_Storage nodes:_
| *CPU:*
| 2 &times; Intel&reg; Xeon&reg; E5-2620v4 2.1GHz 8 Core LGA 2011-3
| *RAM:*
| 8 &times; DIMM DDR4-2400 32GB ECC REG DR
| *HDD:*
| 4 &times; Samsung SSD SM863 960GB, 14 &times; Hitachi 3TB SATA 64MB 7200rpm
| *Network:*
| 2 &times; 1GBit/s Ethernet
|===

On this infrastructure we set up 24 virtual machines. We used six of them to host our core system services except the processing connector--i.e. the JobManager, workflow editor, data access service, data catalogue and service catalogue. We also deployed monitoring and logging services to these machines (see Section <<sec:evaluation-monitoring-and-logging>>). We used the other 18 virtual machines as compute nodes with the processing services and the processing connector installed. They had the following specifications:

[cols="15,85a",frame=none,grid=none]
|===
| *CPU:*
| 2 cores
| *RAM:*
| 8 GiB
| *OS:*
| Linux, Ubuntu 16.04 LTS 64-bit
| *HDD:*
| 2 &times; 160 GiB
|===

Each virtual machine had two virtual hard drives. One drive was used for the operating system, applications (including processing services), and temporary files. The second drive was reserved for the geospatial data we processed. We used GlusterFS to connect all virtual machines and to create a distributed file system with a total capacity of 2.81 TiB (160 GiB &times; 18). In order to introduce redundancy for fault tolerance and to improve read performance, we configured a replication factor of 3 in GlusterFS. Every file we uploaded to the distributed file system was therefore copied to three different virtual machines. We did not enable striping, so a single file was always stored completely on one node and not split into multiple fragments distributed over several nodes.

We deployed and configured the whole system using the IT automation tool Ansible cite:ansible2017[]. The process is further described in Section <<sec:evaluation-deployability>>.

[[sec:evaluation-monitoring-and-logging]]
=== Monitoring and logging

In order to implement distributed logging and monitoring as described in Section <<sec:architecture-operations>>, we set up an infrastructure as depicted in Figure <<img-monitoring-and-logging>>.

[#img-monitoring-and-logging.bottom]
.Distributed logging and monitoring
image::images/06_evaluation/monitoring-and-logging-architecture.pdf[scaledwidth="50%",align="center"]

We deployed an Elastic Stack (formerly known as ELK stack) consisting of Logstash, Elasticsearch and Kibana cite:elastic2017[]. Logstash filtered and transformed incoming log messages from our services and stored them in Elasticsearch where they were analysed and indexed. The web-based graphical user interface Kibana could then be used to query Elasticsearch and to search and aggregate large amounts of log messages by certain criteria. This allowed us to trace distributed workflow executions and to find bugs during development.

In addition to the Elastic Stack, we set up a monitoring chain based on Prometheus cite:prometheus2016[] and Grafana cite:grafana2017[]. Prometheus regularly polled our services for metrics. These metrics included information about the virtual machines our services ran on (CPU, memory, etc.) as well as application-specific values (e.g. the number of process chains currently being executed). We used Grafana to aggregate and visualise the metrics and to create a dashboard with which we could monitor the execution of workflows. The graphs we put into this chapter are screenshots from Grafana.

== Use cases

In order to perform the quantitative evaluation of our system implementation under realistic conditions, we executed workflows from our use cases defined in Chapter <<chap:introduction>> several times under varying conditions. The following two sections describe these workflows and the data sets we applied them to.

[[sec:evaluation-use-case-a]]
=== Use case A: Urban planning

The main tasks in our urban planning use case are to keep cadastral data sets up to date and to monitor the growth of trees in the urban area. For both tasks, experts from the municipality or from a mapping authority analyse large LiDAR point clouds (Light Detection And Ranging) acquired by an LMMS (Laser Mobile Mapping System) and try to extract objects such as urban furniture, traffic lights, façades and trees.

In our evaluation we focused on user story A.2, the extraction of trees from LiDAR data in order to monitor their growth and to foresee pruning works. In order to automate this task, we worked together with domain experts from the national mapping agency of France (Institut Géographique National IGN) and created the following workflow in our Domain-Specific Language:

[source,subs=+quotes]
----
*for each* [PointCloud] *do*
    *apply* Dimensionality
        *using* MinNeighborhood: 64
            *and* MaxNeighborhood: 256
            *and* WindowSize: 0

    *apply* Classification
	
    *apply* PointCloudAttributeToTreeClass

    *apply* IndividualTrees

    *store*
*end*
----

The workflow operates on a number of point cloud tiles acquired by an LMMS. It applies four processing services and then stores the result for each tile to the distributed file system.

Note that we used the generic `apply` keyword (see Section <<sec:use-case-b-dsl-grammar>>) to call individual processing services instead of high-level expressions such as `select Trees`. This allowed us to have a more fine-grained control over which services are called and to monitor their execution more precisely in our evaluation. The advantage of this will become clearer in use case B in Section <<sec:evaluation-use-case-b>>, where we correlate the recorded metrics to service calls in the workflow. Nevertheless, high-level expressions would have yielded the same results.

The workflow contains four processing services created by software developers from different institutions--i.e. University College London UCL, Delft University of Technology, and the French national mapping agency IGN. The '`Dimensionality`' service determines the shape of the neighbourhood for each point. It calculates three dimensionality features on spherical neighbourhoods at various radius sizes and determines if a point and its neighbours lie on an edge, a surface or a volume cite:Demantke2011[]. The service can be applied to 3D point clouds from airborne, terrestrial and mobile mapping systems and requires no a priori knowledge about the distribution of vertices.

The calculated dimensionality provides significant hints for the '`Classification`' service. This service identifies tree crowns in point clouds by applying a Random Forest supervised classifier cite:Breiman2001[]. The service adopts a simplified version of the pipeline from citet:Weinmann2014[]. It consists of three steps: a feature selection that reduces the number of features to classify, the actual classification, and a final regularisation step which produces homogeneous labels by selecting an 80% majority label in a neighbourhood cite:boehm-bredif-gierlinger-kraemer-lindenbergh-liu-michel-sirmacek-2016[].

The third service is called '`PointCloudAttributeToTreeClass`'. This service analyses the classified point cloud and divides the points into two classes: tree and non-tree. The decision whether a point belongs to a tree or not is based on a scattering label that is added to the point cloud by the '`Classification`' service. It is assumed that for tree points the scattering value is higher than for non-tree points. The output of '`PointCloudAttributeToTreeClass`' is a new point cloud containing only tree points.

The last service '`IndividualTrees`' divides the remaining set of points into individual objects. It applies a number of heuristics to create minimal non-overlapping bounding boxes around points belonging to the same tree. The final result is a set of points labelled with unique numbers. Points with the same number belong to the same tree.

==== Data set

We applied the workflow to a large data set acquired by an LMMS in the city of Toulouse, France. The data set had the following characteristics:

[cols="35,60a",frame=none,grid=rows]
|===
| *Number of point cloud tiles*
| 529
| *Number of vertices per tile*
| 3,000,000 (some tiles at the borders of the data set had less points)
| *Total number of vertices*
| 1,580,600,752 (1.58 billion)
| *Acquisition time*
| 1 hour and 53 minutes
| *Total volume*
| 120.63 GiB
|===

==== Collected metrics

The following figures show metrics we collected while applying the workflow to our example data set. The whole process took _1 hour and 51 minutes_. Figure <<img-us2-18nodes-cpu>> shows the CPU usage on each of the 18 compute nodes (18 coloured lines) and Figure <<img-us2-18nodes-memory>> the free memory per node. Figures <<img-us2-18nodes-network>> and <<img-us2-18nodes-disk>> show the network usage and the disk usage respectively, summed up over all of the 18 compute nodes.

[#img-us2-18nodes-cpu]
.Use case A: CPU usage
image::images/06_evaluation/us2/18nodes/cpu.png[scaledwidth="100%",align="center"]

[#img-us2-18nodes-memory]
.Use case A: Memory usage
image::images/06_evaluation/us2/18nodes/memory.png[scaledwidth="100%",align="center"]

[#img-us2-18nodes-network]
.Use case A: Network RX/TX
image::images/06_evaluation/us2/18nodes/network.png[scaledwidth="100%",align="center"]

[#img-us2-18nodes-disk]
.Use case A: Disk I/O
image::images/06_evaluation/us2/18nodes/disk.png[scaledwidth="100%",align="center"]

Since the processing services in this use case are single-threaded and each virtual machine had 2 CPU cores we set the scheduling queue size of the process chain manager to 2 (see Section&nbsp;<<sec:processing-process-chain-manager>>). This means that the JobManager ran up to 2 processing services on each compute node. The graphs show that our system was able to parallelise the workflow execution and to make use of available computational power.

The green line in Figure <<img-us2-18nodes-network>> (network usage) indicates the number of bytes received per second and the red one the number of bytes transmitted. Both metrics are very close to each other, which indicates that the same number of bytes were received as transmitted. This is due to the fact that the processed data is completely stored on the distributed file system spanning over all compute nodes, and that it is only transferred between the nodes while there is almost no communication with external systems.

The number of bytes read from disk in Figure <<img-us2-18nodes-disk>> is significantly lower than the number of bytes written. This is due to the fact that the individual processing services in this workflow produce large temporary files. For example, the '`Dimensionality`' service reads a point cloud and generates a new one with the same number of points but additional attributes. The same is true for the '`Classification`' service that adds labels and further attributes to each point.

[[sec:evaluation-use-case-b]]
=== Use case B: Land monitoring

In our second use case, expert users from the environmental department of the Liguria Region in Italy monitor the evolution of the region's terrain and try to derive knowledge that helps them prepare for future critical events such as landslides and floods. In this chapter we focus on the preparation of LiDAR data acquired by airborne laser scanning.

In order to study the terrain in the region, the expert users would like to select one or more drainage basins and visualise them as 3D surfaces. Our example data set (see below) is irregular. Each point cloud covers a strip of terrain (along the route of the airplane that carried the laser scanner). The length of these strips, the size of the point clouds as well as the covered area varies. Figure <<img-ls1-vector-layer-partitioning>> shows that the strips and the drainage basins do not necessarily match. In order to generate a 3D surface, we first need to analyse the points in all strips and identify which drainage basin they belong to. In doing so, we create an index of points and basins. We then need to convert the points to 3D surfaces by applying a Delaunay triangulation. We generate multiple resolutions (levels of detail) of these surfaces from which the users can select, depending on their requirements for the visualisation.

[#img-ls1-vector-layer-partitioning]
.An example of a point cloud strip that touches three drainage basins
image::images/06_evaluation/ls1/VLP2.png[scaledwidth="60%",align="center"]

The workflow we created together with users from the environmental department of the Liguria Region ("`Regione Liguria`") consists of two parts. The first part creates the index and reorders points according to their saliency. In the second part the expert users can select a specific level of detail and create a triangulation constrained to a certain drainage basin.

The workflow requires two input data sets: a collection of point clouds in the LAS format, as well as a Shapefile containing geometries representing the boundaries of each drainage basin in the region. The first part of the workflow is defined as follows in our Domain-Specific Language:

[source,subs=+quotes]
----
*for each* [PointCloud] *do*
    *apply* ResamplingOfPointCloud
        *using* resamplingResolution: 20

    *apply* OutlierClassificationInPointCloud
        *using* outlierFilteringK: 15
        *and* outlierFilteringStddev: 3.0

    *apply* VectorLayerPointCloudPartitioning
        *with* [boundaries]
*end as* results94

*apply* VLJsonMerger *with* results94 *as* merged94

*for each* merged94.outputMetaFolder *do*
    *apply* MultiresolutionTriangulation
*end as* results48

*apply* MTlasMerger *with* merged94.outputLasFolder

*store*
----

The workflow script contains two '`for`' expressions. The first one iterates over the collection of point clouds and reduces their resolution with the '`ResamplingOfPointCloud`' service. After that, it applies the '`OutlierClassificationInPointCloud`' service which removes outliers--presumably measuring errors--from the resampled point clouds. This ensures that the resulting triangulation is smooth and resembles the actual terrain. The third service '`VectorLayerPointCloudPartitioning`' splits an input point cloud along the drainage basin boundaries. The service creates multiple files, one for each basin containing points from the input data set. For example, the strip shown in Figure <<img-ls1-vector-layer-partitioning>> would be divided into three files. The service also creates a JSON file for each generated point cloud containing metadata necessary for the services in the subsequent steps.

Since the point clouds are processed in parallel, multiple instances of '`VectorLayerPointCloudPartitioning`' may create different files for drainage basins covering more than one point cloud. The result of the first '`for`' expression is hence a list of files. The service '`VLJsonMerger`' accepts this list as input and merges metadata files belonging to the same basin to single files.

In a second '`for`' expression the partitioned point clouds are then processed by the '`MultiresolutionTriangulation`' service. This service orders points in a hierarchical manner according to their saliency--i.e. how much they contribute to the appearance of the terrain.

The final service '`MTlasMerger`' merges point clouds belonging to the same basin to a single file. After the first part of the workflow has finished, users may choose to run a subsequent part that looks as follows:

[source,subs=+quotes]
----
*for each* [basins] *do*
    *apply* LodExtract
        *using* lodOfInterest: 10
        *as* extractedLod

    *apply* ConstrainedTringulation
        *with* extractedLod *and* [boundary]
*end as* results

*store* results
----

In this workflow script, users can define a certain level of detail they want to extract from the point clouds prepared in the first part. The '`LodExtract`' service works on the points reordered by the '`MultiresolutionTriangulation`' service and selects the most salient ones until it reaches a resolution that satisfies the given level of detail. After that, the '`ConstrainedTringulation`' service performs a Delaunay triangulation and constrains the result to the drainage basin boundaries according to citet:Shewchuk1996[]. The whole process can be applied to multiple basins in parallel. The final results are stored in the distributed file system.

Note that in the following we focus on the first part of the workflow only. It covers a range of functionalities provided by our system and is therefore suitable to evaluate it. The second one, on the other hand, is typically only applied to a small portion of the data. It is not very complex and typically finishes in a short time. Performing tests with it would not reveal additional insights.

==== Data set

As described earlier, our workflow requires two input data sets. A collection of point cloud strips acquired by airborne laser scanning and a Shapefile containing the boundaries of all drainage basins in the Liguria region. In order to evaluate our system, we used a point cloud collection with the following properties:

[cols="35,60a",frame=none,grid=rows]
|===
| *Number of point cloud strips*
| 684
| *Total number of points*
| 17,345,032,915 (17.35 billion)
| *Total volume*
| 451.16 GiB
|===

In addition, we used a Shapefile with a total of 638 drainage basin boundaries. Figure <<img-liguria-point-cloud-strips>> shows the areas covered by the point cloud strips. For a map of the drainage basin boundaries, we refer to Figure pass:[<xref linkend="img-liguria-drainage-basins" xrefstyle="select: labelnumber pageabbrev"/>].

[#img-liguria-point-cloud-strips]
.A map showing the areas covered by the point cloud strips in our example data set
image::images/06_evaluation/ls1/liguria-las.png[scaledwidth="72%",align="center"]

==== Collected metrics

The metrics we collected while applying the land monitoring workflow to our example data set are shown in Figures <<img-ls1-18nodes-cpu>> to <<img-ls1-18nodes-processchains>>. Similar to use case A, we collected CPU usage, memory usage, network traffic, and disk I/O. In addition, we recorded the number of available compute slots (which is the number of compute nodes multiplied by the size of the scheduling queue, see Figure&nbsp;<<img-ls1-18nodes-available-nodes>>) as well as the number of process chains the JobManager had to execute at a certain point in time (Figure <<img-ls1-18nodes-processchains>>). The whole process took _35 minutes and 49 seconds_.

[#img-ls1-18nodes-cpu]
.Use case B: CPU usage
image::images/06_evaluation/ls1/18nodes/cpu_phases_top.png[scaledwidth="95%",align="center"]

[#img-ls1-18nodes-memory]
.Use case B: Memory usage
image::images/06_evaluation/ls1/18nodes/memory.png[scaledwidth="95%",align="center"]

[#img-ls1-18nodes-network]
.Use case B: Network RX/TX
image::images/06_evaluation/ls1/18nodes/network.png[scaledwidth="95%",align="center"]

[#img-ls1-18nodes-disk]
.Use case B: Disk I/O
image::images/06_evaluation/ls1/18nodes/disk.png[scaledwidth="95%",align="center"]

[#img-ls1-18nodes-available-nodes]
.Use case B: Available compute slots
image::images/06_evaluation/ls1/18nodes/availablenodes_phases_top.png[scaledwidth="95%",align="center"]

[#img-ls1-18nodes-processchains]
.Use case B: Number of process chains to execute
image::images/06_evaluation/ls1/18nodes/processchains.png[scaledwidth="95%",align="center"]

In order to be able to better compare the figures and to correlate them with the workflow script, we created labels showing the four phases of the workflow: the first '`for`' expression, the first merge step, the second '`for`' expression, and the final merge step. The figures show that the CPU usage is very high in the first phase and that all compute nodes are almost completely used to their capacity. From Figure <<img-ls1-18nodes-processchains>> we can see that the JobManager's rule system has created as many process chains as there are input point cloud strips and that they are constantly being processed. Figure <<img-ls1-18nodes-available-nodes>> shows that these process chains are distributed to all compute nodes.

The CPU usage is not constantly at 100% in the first phase because there is also a lot of I/O at the same time. The point cloud strips are read from the distributed file system and transferred over the network to peer compute nodes. Note that the number of bytes transferred over the network per second is lower than the number of bytes read from disk. This is due to the fact that the JobManager is able to run processing services on nodes that actually contain the files to be processed. Since the JobManager is optimised for maximum throughput and our scheduling queue has a length of 2 only, this optimisation does not work all the time. There is a trade-off between the number of process chains the JobManager can execute in a certain amount of time and the overhead introduced by the amount of data transferred over the network. However, as we will show in Section <<sec:evaluation-performance>>, this has almost no effect on the overall performance of the workflow.

The second workflow phase represents the first merge operation. The figures show that only one virtual machine is occupied in this phase. CPU usage is not very high because the '`VLJsonMerger`' service is single-threaded. In addition, it has to read a very large number of small files generated by the '`VectorLayerPointCloudPartitioning`' service, which introduces latency not visible in the figures.

In the third phase the JobManager processes the second '`for`' expression. Similar to the first one, there are a lot of process chains to be executed. They are distributed evenly to the compute nodes. Except for a few, the individual process chains do not take very long to run. This is the reason why Figure <<img-ls1-18nodes-cpu>> shows a spike in the CPU usage at the beginning of the phase and a longer period where only a few nodes are used to a maximum of 50%. The network traffic and disk I/O are relatively low in this phase because the original input files have already been reduced in size.

The final phase contains the last merge operation. Similar to '`VLJsonMerger`' the '`MTlasMerger`' service is single-threaded and can run on one compute node only. Again, the latency introduced by the fact that the service has to process a large number of very small files leads to a small CPU usage and almost negligible spikes in the figures for network traffic and disk I/O.

[[sec:evaluation-quality-attributes]]
== Quality attributes

In this section we report on the results of the qualitative evaluation of our system against the quality attributes we defined in Section <<sec:architecture-quality-attributes>>. For each quality attribute, there is a general scenario describing an event, a system response, as well as measurable criteria to validate if our system actually responded as expected. For the evaluation we derived specific scenarios from the general ones which described how the system was expected to respond if it was applied to our use cases. We then performed the validation and recorded metrics. In this section we present the specific scenarios and discuss the validation results.

[[sec:evaluation-performance]]
=== Performance

In Section <<sec:architecture-quality-attributes>> we specified two criteria to define what '`performance`' means for our system (see Table pass:[<xref linkend="tab:architecture-quality-attribute-performance" xrefstyle="select: labelnumber Page"/>]). The first criterion referred to the execution time of a workflow, and the second one to Cloud resource usage.

==== Execution time

In our urban use case we require that the time it takes to process a large LiDAR point cloud should be equal to or less than its acquisition time. We derived a specific scenario from this requirement in order to be able to evaluate if our system is fast enough (see Table <<tab:evaluation-quality-attribute-performance-a>>).

[[tab:evaluation-quality-attribute-performance-a]]
.Performance Specific Scenario A - Time constraints
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Users and GIS experts from municipalities, mapping authorities

| *Stimulus*
| Automatic tree detection within large point clouds acquired by an LMMS

| *Environment*
| Normal operation

| *Artefacts*
| Whole system

| *Response*
| The system processes the acquired data (i.e. it identifies individual trees) and offers the results for download

| *Response measure*
| The time needed to process the data is equal to or less than the acquisition time
|===

Comparing the acquisition time of our example LiDAR data set from Section <<sec:evaluation-use-case-a>> (_1 hour and 53 minutes_) with the processing time on 18 virtual machines shown in Figure <<img-us2-18nodes-cpu>> (_1 hour and 51 minutes_) we can clearly state that our goal from use case A has been reached. It is indeed possible to process the data set in less time than it took to acquire it.

As we will show in Section <<sec:evaluation-scalability>> the workflow scales almost linearly. The performance could therefore be even more improved by adding further virtual compute nodes. Our end-users suggested to insert another processing service to the workflow which resamples the data to a lower resolution before it is processed by the other services (i.e. the '`ResamplingOfPointCloud`' service, see Section <<sec:evaluation-use-case-b>>). This would have reduced the processing time even further and--with a reasonable resampling factor--would have yielded almost the same results.

==== Resource usage

In order to evaluate whether our system makes best use of available resources, we defined a specific scenario as described in Table <<tab:evaluation-quality-attribute-performance-b>>.

[[tab:evaluation-quality-attribute-performance-b]]
.Performance Specific Scenario B - Resource usage
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Users and GIS experts

| *Stimulus*
| Processing of aerial point clouds for land monitoring

| *Environment*
| Normal operation

| *Artefacts*
| Whole system

| *Response*
| The system processes the data and offers the results for download

| *Response measure*
| While processing, the system makes best use of available resources

_a)_ All compute nodes are used to their full capacity in terms of CPU power

_b)_ The amount of data transferred over the network is minimised
|===

Response measure a) is satisfied by our system. Figure pass:[<xref linkend="img-ls1-18nodes-cpu" xrefstyle="select: labelnumber page"/>] shows the CPU usage recorded during a workflow run of use case B on 18 compute nodes. For this run we set the size of the scheduling queue in our process chain manager to 2. This means that the JobManager always executed up to two processing services per compute node at the same time. Although the individual services were single-threaded, both CPU cores on each node were fully used.

In order to support a wider range of processing services and workflows, the size of the scheduling queue in the JobManager should be dynamic and vary depending on whether a single-threaded or a multi-threaded processing service is executed. The queue size should also depend on the number of CPU cores available on each compute node. For the experiments presented here, we assume that the processing services are single-threaded. Further, we assume that all compute nodes are homogeneous and have two CPU cores.

In order to validate response measure b), we ran the workflow for use case B another time on 18 compute nodes but disabled the data locality optimisation of the JobManager (see Section&nbsp;<<sec:processing-rule-system>>). This means that the processing services were executed on arbitrary nodes and not on those that host the respective input data. Figure <<img-ls1-nopreferrednodes-network>> and Figure <<img-ls1-nopreferrednodes-disk>> show the network usage and the disk usage for this workflow run respectively. If we compare these two figures to the network usage and disk usage recorded in the first workflow run (see Figure <<img-ls1-18nodes-network>> and pass:[<xref linkend="img-ls1-18nodes-disk" xrefstyle="select: labelnumber Page"/>]) we can state that while the disk usage has not changed, the amount of data transferred over the network is now much higher. This is due to the fact that the same amount of data had to be read from disk during the workflow run, but since the distribution of the processing services across the compute nodes was much more arbitrary, input data from the distributed file system had to be transferred between nodes more often.

[#img-ls1-nopreferrednodes-network]
.Use case B: Network usage with disabled data locality optimisation
image::images/06_evaluation/ls1/preferrednodes_nopreferrednodes_network.png[scaledwidth="98%",align="center"]

[#img-ls1-nopreferrednodes-disk]
.Use case B: Disk usage with disabled data locality optimisation
image::images/06_evaluation/ls1/preferrednodes_nopreferrednodes_disk.png[scaledwidth="98%",align="center"]

The workflow run with disabled data locality optimisation took _35 minutes and 1 second_. This is almost identical to the first workflow run from Section <<sec:evaluation-use-case-b>> which took _35 minutes and 49&nbsp;seconds_. The difference is within the accuracy of measurement and subject to fluctuations that are to be expected in a dynamic Cloud environment. One would expect that the workflow execution without locality optimisation is much slower since more data has to be transferred over the network. However, in our case the network is much faster than the disk and hence the disk speed affects the execution time more than the amount of data transferred over the network.

Note that we performed our evaluation in a controlled and isolated environment. The results may vary on other Cloud infrastructures where the data is not stored in the same rack and probably even not located in the same data centre. In this case data would have to be transferred over a much slower network connection (possibly a WAN connection). Our graphs show that network traffic is reduced by our optimisation and that it is actually in effect. The impact on execution time will be more evident in set ups involving slower network connections.

[[sec:evaluation-scalability]]
=== Scalability

In Section <<sec:architecture-quality-attributes>> we specified a general scenario for the quality attribute _scalability_ with three response measures:

* The system should continue to work under heavy load (multiple workflow executions at the same time)
* Workflows should be processed faster with more computational resources available
* The system should be able to handle arbitrary amounts of data

In this section we present specific scenarios for these three measures and report on the results of experiments we performed on the Cloud with our example workflows and data sets.

==== Multiple workflow executions

We first tested the execution of concurrent workflows over a defined period of time in order to evaluate how our system performs under high load. The specific scenario for this test is given in Table <<tab:evaluation-quality-attribute-scalability-a>>.

[[tab:evaluation-quality-attribute-scalability-a]]
.Scalability Specific Scenario A - Multiple workflow executions
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Users, GIS experts, data providers

| *Stimulus*
| Execution of concurrent workflows

| *Environment*
| Overloaded operation

| *Artefacts*
| Whole system

| *Response*
| The system executes all submitted workflows

| *Response measure*
| 1) All workflows complete as if they were executed one after the other under normal operation

2) The workflows are finished in reasonable time
|===

We executed the workflow for our use case A twelve times on 18 virtual machines with a scheduling queue size of 2 in the process chain manager. We timed the executions so that we could simulate concurrent runs at the same time and high load over a longer period. The following table shows which workflow run was started at which point in time.

[cols="5,10,10,10,10,10,10",frame=none,grid=rows]
|===
| *Run* | #1 | #2 | #3 | #4 | #5 | #6
| *Start* | 0h 00m 00s | 0h 00m 05s | 0h 01m 05s | 2h 01m 05s | 2h 31m 05s | 2h 31m 10s
|===

[cols="5,10,10,10,10,10,10",frame=none,grid=rows]
|===
| | #7 | #8 | #9 | #10 | #11 | #12
| | 2h 31m 11s | 2h 31m 12s | 2h 31m 17s | 2h 41m 17s | 8h 41m 17s | 11h 41m 17s
|===

Figure <<img-us2-multiple_users-processchains>> shows the number of process chains generated during the experiment. At the beginning there is a quick ramp up as three workflows were executed within 1 minute. The maximum number of process chains in the JobManager's queue was reached at about 2h 41m, after the first ten workflow runs had been started. From there on, there was a constant decline with two more spikes for workflow runs 11 and 12.

Figures <<img-us2-multiple_users-cpu>> to <<img-us2-multiple_users-disk>> depict the Cloud resource usage over the whole period of time. The graphs show values very similar to the ones collected during our first run in Section <<sec:evaluation-use-case-a>>. There are no specific peaks or other anomalies. This indicates that even under high load our system continued to operate as if it was executing only one workflow.

The whole experiment took _21 hours and 50 minutes_. Twelve sequential runs of our urban workflow would have taken _22 hours and 12 minutes_ (1h 51m &times; 12). The concurrent runs were hence a bit faster. This is due to the fact that the JobManager could leverage parallelisation better, in particular at the end of each run where only a few process chains were left and a couple of compute nodes became already available again.

In summary, we can state that our system did not show any signs of overload during the experiment. It also stayed responsive all the time. Workflows could be started without any noticeable latency and metrics could continuously be retrieved from the JobManager and the compute nodes. This is primarily attributable to the fact that we designed the JobManager to be reactive (as described in Section <<sec:processing-overview>>) and that the individual microservices running in our system are isolated and do not affect each other's performance.

[#img-us2-multiple_users-processchains]
.Concurrent executions of use case A: Number of process chains
image::images/06_evaluation/us2/multiple_users/processchains.png[scaledwidth="100%",align="center"]

[#img-us2-multiple_users-cpu]
.Concurrent executions of use case A: CPU usage
image::images/06_evaluation/us2/multiple_users/cpu.png[scaledwidth="100%",align="center"]

[#img-us2-multiple_users-memory]
.Concurrent executions of use case A: Memory usage
image::images/06_evaluation/us2/multiple_users/memory.png[scaledwidth="100%",align="center"]

[#img-us2-multiple_users-network]
.Concurrent executions of use case A: Network RX/TX
image::images/06_evaluation/us2/multiple_users/network.png[scaledwidth="100%",align="center"]

[#img-us2-multiple_users-disk]
.Concurrent executions of use case A: Disk I/O
image::images/06_evaluation/us2/multiple_users/disk.png[scaledwidth="100%",align="center"]

==== Number of compute nodes

Another aspect of scalability that we evaluated was how our system would react to various numbers of compute nodes and how the workflows from our two use cases would perform. The specific scenario given in Table <<tab:evaluation-quality-attribute-scalability-c>> describes the test we performed.

[[tab:evaluation-quality-attribute-scalability-c]]
.Scalability Specific Scenario B - Number of compute nodes
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Cloud infrastructure, administrators

| *Stimulus*
| New compute nodes are made available to the system

| *Environment*
| Normal operation

| *Artefacts*
| Whole system

| *Response*
| The system makes use of the capacity offered by the added compute nodes

| *Response measure*
| The time needed to execute a workflow decreases with the number of nodes added

|===

We applied this scenario to both use cases. We executed the workflows on 6, 9, 12, 15 and 18 compute nodes. On each node we repeated the test five times to calculate mean and median. Table&nbsp;<<tab:evaluation-scalability-us2>> shows the results for use case A. The median values for each node are plotted in Figure&nbsp;<<img-us2-scalability>> in a semi-logarithmic scale.

[[tab:evaluation-scalability-us2]]
.Execution times of use case A on a varying number of compute nodes
[cols="20,20,20,20,20,20",frame=none,grid=rows,options="header"]
|===
|           | 6 nodes  | 9 nodes  | 12 nodes | 15 nodes | 18 nodes
| *Run #1*  | 4h 54m   | 3h 19m   | 2h 33m   | 2h 10m   | 1h 53m
| *Run #2*  | 4h 55m   | 3h 19m   | 2h 32m   | 2h 11m   | 1h 51m
| *Run #3*  | 4h 55m   | 3h 19m   | 2h 34m   | 2h 09m   | 1h 53m
| *Run #4*  | 4h 57m   | 3h 19m   | 2h 32m   | 2h 10m   | 1h 54m
| *Run #5*  | 4h 56m   | 3h 20m   | 2h 32m   | 2h 09m   | 1h 53m
| *Mean*    | _4h 55m_ | _3h 19m_ | _2h 33m_ | _2h 10m_ | _1h 53m_
| *Median*  | _4h 55m_ | _3h 19m_ | _2h 32m_ | _2h 10m_ | _1h 53m_
|===

[#img-us2-scalability]
.Median of the execution times of use case A on a varying number of compute nodes (semi-logarithmic)
image::images/06_evaluation/us2/scalability.png[scaledwidth="100%",align="center"]

We can state that the workflow scales almost linearly. For example, the run on 12 nodes takes almost half as long as the one on 6 nodes. The graph shows, however, a slight curve indicating that there is going to be a point where the I/O overhead will be too high and the performance will not improve even if more compute nodes are added. Looking at the DSL script for this workflow again, the reason for this behaviour becomes apparent. There is only one '`for`' expression. Since all iterations are independent from each other, they can theoretically be executed in parallel. The only limitation seems to be the number of compute nodes that can process tasks in parallel at a certain point in time. In theory, the point where performance could not be improved any more by merely adding compute nodes would be at 529, which is the number of input point clouds. In practise, overhead caused by I/O and scheduling in the JobManager leads to a slight degradation.

We performed a similar test for use case B. Table <<tab:evaluation-scalability-ls1>> shows the collected timings and Figure&nbsp;<<img-ls1-scalability>> the plotted median values. Use case B performs slightly worse than use case A in terms of scalability. The figure shows a clear curve progression. There is actually no difference between the execution times on 15 and 18 nodes, which indicates that 15 nodes are the optimum and performance cannot be improved any more beyond this point. This behaviour is firstly due to the overhead of I/O operations and scheduling. The workflow reads almost all data at the beginning (in the first '`for`' expression) and works with a large number of very small files later on. Secondly, the behaviour is also to a large extent caused by the fact that the two merge operations are single-threaded and cannot be parallelised. Even if more compute nodes are added, the merge operations will take the same time. The only possibility to improve performance would be to scale vertically which means adding better hardware components (i.e. faster CPUs and hard drives).

[[tab:evaluation-scalability-ls1]]
.Execution times of use case B on a varying number of compute nodes
[cols="20,20,20,20,20,20",frame=none,grid=rows,options="header"]
|===
|           | 6 nodes   | 9 nodes   | 12 nodes  | 15 nodes  | 18 nodes
| *Run #1*  | 47m 55s   | 41m 52s   | 37m 48s   | 35m 31s   | 36m 29s
| *Run #2*  | 48m 28s   | 40m 32s   | 37m 27s   | 35m 36s   | 35m 49s
| *Run #3*  | 48m 00s   | 40m 27s   | 37m 07s   | 35m 11s   | 35m 26s
| *Run #4*  | 47m 39s   | 40m 15s   | 37m 30s   | 35m 16s   | 35m 05s
| *Run #5*  | 47m 40s   | 40m 44s   | 37m 01s   | 36m 25s   | 35m 02s
| *Mean*    | _47m 56s_ | _40m 46s_ | _37m 22s_ | _35m 36s_ | _35m 34s_
| *Median*  | _47m 55s_ | _40m 32s_ | _37m 27s_ | _35m 31s_ | _35m 26s_
|===

[#img-ls1-scalability]
.Median of the execution times of use case B on a varying number of compute nodes (semi-logarithmic)
image::images/06_evaluation/ls1/scalability.png[scaledwidth="100%",align="center"]

==== Data volume

The third response measure for our scalability evaluation is data volume. Table <<tab:evaluation-quality-attribute-scalability-b>> describes a specific scenario for this.

[[tab:evaluation-quality-attribute-scalability-b]]
.Scalability Specific Scenario C - Data volume
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Users, GIS experts, data providers

| *Stimulus*
| Execution of workflows to process arbitrarily large data volumes

| *Environment*
| Normal operation

| *Artefacts*
| Whole system

| *Response*
| The system successfully executes the workflows

| *Response measure*
| The system does not become overloaded and continues to operate normally. The system's behaviour is the same regardless of how much data it has to process.

|===

Above, we have already shown that our system is able to process arbitrary sizes of data. When we tested concurrent workflow executions earlier, we processed the whole data set of use case A twelve times. This makes a total of 120.63 &times; 12 = 1,41 TiB of data. We have shown that the system behaves similar to a single workflow run. This is due to the fact that our system does not interact with data directly. In order to find limitations, one would have to evaluate single processing services and determine their scalability. We have done such an evaluation within the IQmulus research project cite:iqmulus_d_6_5[prefix=see].

[[sec:evaluation-availability]]
=== Availability

In order to evaluate whether our system can continue to operate in case of faults occurring during a workflow run, we defined the specific scenario given in Table <<tab:evaluation-quality-attribute-availability>>.

[[tab:evaluation-quality-attribute-availability]]
.Availability Specific Scenario
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Network

| *Stimulus*
| Random refused connections and timeouts

| *Environment*
| Unstable operation

| *Artefacts*
| Whole system

| *Response*
| The system successfully executes the workflow from use case B

| *Response measure*
| 1) The system should still be able to finish the workflow execution, even if there is a fault

2) The workflow execution might take longer as usual but should produce the same results.

|===

We executed the workflow from our use case B on 18 compute nodes and simulated a very unreliable network by randomly blocking connections to compute nodes with the Linux kernel firewall. We used `iptables` to configure the firewall. For example, the following command adds a rule to block outgoing TCP connections to a node with the IP address `192.168.0.26`:

----
iptables -A OUTPUT -p tcp -d 192.168.0.26 -j DROP
----

Figure <<img-ls1-availability-recoverableerrors>> shows the cumulative number of errors from which the JobManager was able to recover--i.e. it was able to detect an error and to reschedule work to another available compute node. The chart shows errors that occurred during the submission of process chains to the Processing Connector and during status polling.

[#img-ls1-availability-recoverableerrors]
.Use case B: Recoverable errors
image::images/06_evaluation/ls1/availability_recoverableerrors.png[scaledwidth="100%",align="center"]

[#img-ls1-availability-circuitbreaker]
.Use case B: Circuit breaker states per node
image::images/06_evaluation/ls1/availability_circuitbreaker.png[scaledwidth="100%",align="center"]

As described in Section <<sec:processing-fault-tolerance>>, we implemented the Circuit Breaker pattern to avoid unnecessary calls to compute nodes that are currently unavailable. Figure <<img-ls1-availability-circuitbreaker>> shows the states of the circuit breakers per compute node. The figure is a matrix with 18 rows (one row per node). Each of these rows represents a circuit breaker state over time. The green colour means the circuit breaker for the respective node was closed (connections were permitted). Red means the circuit breaker was open (connections were not allowed), and yellow means the circuit breaker was in half open state.

We configured each circuit breaker to close when its node could not be reached two or more times. After 60 seconds the circuit breaker should return to the half open state. From there it should either immediately return to the open state if the node was still unavailable, or change to the closed state if the first connection attempt was successful.

The whole workflow run took _41 minutes and 46 seconds_. This is about 6 minutes slower than the average run time from Section <<sec:evaluation-scalability>>. However, the workflow run was successful and produced the same results as with a more reliable network connection. The JobManager was able to reschedule process chains if the compute nodes they were assigned to became unavailable.

Note that in Figure <<img-ls1-availability-circuitbreaker>> five circuit breakers stayed in the half open state until the end of the workflow and beyond. This is due to the fact that circuit breakers only return to the closed state if there was at least one successful connection attempt. Since there was only one process chain to be executed at the end of the workflow (the second merge operation) no further connection attempts were made and the circuit breakers stayed half open until more work was assigned to their nodes in subsequent workflow runs.

[[sec:evaluation-modifiability]]
=== Modifiability

For the modifiability quality attribute we identified three requirements in Section <<sec:architecture-quality-attributes>>:

1. The users want to control how our system processes data (i.e. they want to create their own custom workflows)

2. The members of the system development team want to add new features (i.e. modify the system)

3. Developers of geospatial algorithms want to integrate their processing services into our system

==== Creating custom workflows

In order to allow users to create custom workflows for geospatial processing, we created a Domain-Specific Language and implemented a workflow editor in Chapter <<chap:workflow-modelling>>. Table <<tab:evaluation-quality-attribute-modifiability-a>> describes a specific scenario related to this.

[[tab:evaluation-quality-attribute-modifiability-a]]
.Modifiability Specific Scenario A - Creating custom workflows
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Users

| *Stimulus*
| Create a custom workflow to process data

| *Environment*
| Runtime

| *Artefacts*
| Workflow editor

| *Response*
| The custom workflow is stored in the system and is available for execution

| *Response measure*
| It should be possible to create custom workflows for different use cases and execute them
|===

This specific scenario can be realised with our system. We created workflows for use cases A and B in Section <<sec:evaluation-use-case-a>> and <<sec:evaluation-use-case-b>> respectively. We presented the workflow scripts in our Domain-Specific Language and executed the workflows to demonstrate that our system is actually able to translate them to runnable process chains. A qualitative discussion on our Domain-Specific Language is given below in Section <<sec:evaluation-stakeholder-requirements>>.

==== Modifying the system

In order to facilitate modifiability, we designed our system based on the microservice architectural style. Microservices are loosely coupled and run as separate processes. This should allow system developers to modify and redeploy a single service without having to interrupt the operation of the rest of the system. We used the specific scenario in Table <<tab:evaluation-quality-attribute-modifiability-b>> to evaluate this quality attribute.

[[tab:evaluation-quality-attribute-modifiability-b]]
.Modifiability Specific Scenario B - Modifying the system
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| System developers, integrators

| *Stimulus*
| Add, remove or modify functionality to the system. Change technologies, modify configurations, etc.

| *Environment*
| Compile time, build time, runtime

| *Artefacts*
| Code, interfaces, configurations

| *Response*
| The modification is made and deployed

| *Response measure*
| It should be possible to make modifications to the system without having to rebuild and redeploy it as a whole
|===

As described in Section <<sec:introduction-use-cases>>, we applied our approach in practise within the IQmulus research project. Software development in this project was very agile. We deployed an initial prototype of our system to the Cloud early on, so that users could test it and provide feedback. The prototype was continuously improved. In order to allow users to regularly test the system, we kept it running until the end of the project (and further) without any notable downtime.

In busy phases of the project we deployed new features and improvements several times per day. The microservice architecture was very beneficial for us because it allowed us to modify one or more services and to deploy new versions during runtime without the users even noticing it.

The only issue we had was that, initially, we could not update the JobManager while a workflow was running. We had to deploy the component redundantly and introduce mechanisms for fault-tolerance as well as the possibility to resume workflows after a restart. One of the keys to this was the fact that the JobManager is stateless and that information about running workflows and process chains are kept persistently in a database.

On the other hand, due to the fact that our microservices ran in isolated processes, we could modify any service but the JobManager at any time and redeploy it even if a workflow was currently running. We were even able to replace processing services used in a workflow while it was being executed. This was a great benefit for us that would not have been possible with a monolithic system. For further information on the evaluation of the automatic deployment of our system components see Section&nbsp;<<sec:evaluation-deployability>>.

==== Integrating new processing services

One of the core requirements from the processing service developers we worked with was that it should be possible to integrate new services into the system, without having to modify the actual code of the system or to interrupt its operation (see Table <<tab:evaluation-quality-attribute-modifiability-c>>).

[[tab:evaluation-quality-attribute-modifiability-c]]
.Modifiability Specific Scenario C - Integrating new processing services
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Processing service developers

| *Stimulus*
| Add, remove or modify a processing service

| *Environment*
| Runtime

| *Artefacts*
| Processing services

| *Response*
| The new version of the processing service is deployed

| *Response measure*
| It should be possible to add, remove or modify processing services without having to rebuild and redeploy the whole system
|===

As described above, in the IQmulus research project we deployed a prototype of our system early on and gradually added geospatial processing services. At the end of the project we had integrated a total of 88 processing services which had been developed by partners distributed over various European countries (see Sections <<sec:evaluation-development-distributability>> and <<sec:evaluation-deployability>>). Many of these services were improved and updated over the time and therefore deployed many times during normal operation of our system. As mentioned above, we were able to redeploy fixed versions of erroneous processing services used in a workflow while it was running. All the service developers had to do for this was to upload their service together with updated metadata to our artefact repository (see Section&nbsp;<<sec:architecture-artefact-repository>>). The JobManager automatically picked new service versions up from the repository and deployed them to the Cloud.

[[sec:evaluation-development-distributability]]
=== Development distributability

One of the core requirements specified in Chapter <<chap:architecture>> was that it should be possible that distributed teams work on different components of our architecture and integrate them at a central location to a running system. The quality attribute that describes this requirement is _development distributability_&mdash;the possibility to distribute software development to independent teams. We validated that our system implementation has this quality attribute within the IQmulus research project. The specific scenario for development distributability (Table <<tab:evaluation-quality-attribute-development-distributability>>) reflects this.

[[tab:evaluation-quality-attribute-development-distributability]]
.Development Distributability Specific Scenario
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Developers working in the IQmulus project

| *Stimulus*
| Develop system components in distributed teams

| *Environment*
| &ndash;

| *Artefacts*
| Processing services, core system services

| *Response*
| Core system service and processing services act together and form our system (in this case the IQmulus platform)

| *Response measure*
| Independent teams from 12 institutions distributed over 7 European countries can develop software components and integrate them into our system on their own
|===

As stated earlier, in the IQmulus project we integrated a total of 88 processing services. In addition, we developed 7 core system services and deployed at least 9 external services. We had 12 different institutions from 7 European countries working on the project.

The software development was structured into teams. We worked according to the "`You build it, you run it`" principle cite:OHanlon2006[] which means that the teams had a defined set of services for which they were responsible which not only included software development and bug fixing, but also operational aspects such as the integration into our system and the deployment. They also had direct contact to the end-users in our project and were involved in the requirements analysis as well as in support. This allowed them to build services that met the requirements of the people who applied them later in their workflows. In addition, the teams could react quicker to changing customer requirements or reported bugs and deploy updated versions of their services independently and at any time.

The teams were structured into groups working together on workflows from three domains: land, urban and marine. At the end of the project we realised nine different use cases with our system. Most of the processing services were only used in a specific use case, but some of them--the more generic ones--were also used multiple times across workflows.

The fact that teams worked independently and that there was no single person who integrated all services into the system required high discipline and caused communication effort that should not be underestimated. For example, we had to have bi-weekly or sometimes weekly telephone conferences, as well as separate online meetings and regular in-person meetings to coordinate the component integration. However, we believe that the efforts were still less than in a monolithic system and that we were able to use the microservice architectural style to our advantage. In a monolithic system there are often dependencies between components that grow stronger over time until they cannot be removed any more. Two dependent components then essentially become one, and the monolith becomes harder to maintain. With our microservice architecture we were able to keep flexibility and maintainability as well as independence between teams from the beginning of the software development until the end of the project (and beyond).

[[sec:evaluation-deployability]]
=== Deployability

Similar to the development distributability quality attribute, the specific scenario for the evaluation of the deployability of our system is defined within the context of the IQmulus research project (Table <<tab:evaluation-quality-attribute-deployability>>).

[[tab:evaluation-quality-attribute-deployability]]
.Deployability Specific Scenario
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| System developers and processing service developers working in the IQmulus project

| *Stimulus*
| Deploy the whole system, update single services, or change configuration

| *Environment*
| Initial deployment, normal operation

| *Artefacts*
| Whole system, 104 individual services, configuration

| *Response*
| The system is fully operational

| *Response measure*
| 1) The deployment process is fully automated

2) All 104 services are up and running

3) The modified configuration is in effect

|===

As mentioned earlier, in the IQmulus project we had a total of 88 processing services. In addition, we developed 7 core system services: the main user interface, the data access service, the workflow editor, the workflow service, the catalogue service (consisting of data catalogue and service catalogue), the JobManager, as well as the processing connector. We also deployed 9 external services. This includes services for monitoring and logging such as Prometheus, Grafana, Logstash, Elasticsearch, Kibana, as well as the distributed file system GlusterFS, Hadoop, Spark, and our artefact repository Artifactory.

In summary, we had 104 different components. Many of them had to be deployed multiple times to different virtual machines in the Cloud (such as the processing connector that runs on every compute node). In addition, the individual services required a lot of configuration. This included system configuration that we had to modify or service-specific configuration files.

As described in Section <<sec:architecture-infrastructure-deployment>>, we used the IT automation tool Ansible cite:ansible2017[] to keep the effort of deploying and maintaining different versions of software components and configurations at a minimum. With this tool, we were able to re-deploy the whole system including all core services, as well as the external ones, with a single command. As described in Section <<sec:architecture-artefact-repository>>, the 88 processing services were stored in our artefact repository and deployed automatically by the JobManager.

Since tasks in Ansible are idempotent, we could re-run the deployment at any time and keep the whole infrastructure in a consistent state. We kept the infrastructure description and the configurations in a code repository and under version control in order to always be able to trace back changes and to revert them if necessary cite:Loukides2012[prefix="see"]. Similarly, the artefact repository Artifactory had a version control system that allowed us to keep track of individual versions of processing services.

In Ansible automated deployment is specified in so-called _playbooks_. One playbook describes a single component. In total, we had 42 playbooks for the system core services, the external services and the configurations. Artifactory hosted 2,207 artefacts. Most of them were different versions of processing services.

The fact that our services were microservices running in separate processes also allowed us to deploy individual services separately. This applied to the core system services and the external services, which we managed with Ansible, but also to the processing services we kept in Artifactory and which were deployed automatically by the JobManager.

=== Portability

The final quality attribute we evaluated was portability. According to the requirements defined in Chapter <<chap:architecture>>, it should be possible to deploy our system to various platforms. We tried to avoid vendor lock-ins and used technologies such as Java and Docker to isolate individual software components and to make them platform-independent. Table <<tab:evaluation-quality-attribute-portability>> describes a specific scenario to evaluate the portability of our system.

[[tab:evaluation-quality-attribute-portability]]
.Portability Specific Scenario
[cols="8,42a",frame=topbot,grid=none]
|===
| *Source*
| Business professionals, customers, IT operations

| *Stimulus*
| The system should be deployed to a certain environment

| *Environment*
| Initial deployment

| *Artefacts*
| Whole system

| *Response*
| The system is fully operational

| *Response measure*
| The system can be deployed to two environments: an OpenStack Cloud and a VMware cluster

|===

We used the automated deployment approach described in the previous Section <<sec:evaluation-deployability>> to implement this specific scenario. First, we deployed the system to the OpenStack Cloud described in Section <<sec:evaluation-environment>>. In addition, during the IQmulus project we deployed three instances of our system to a VMware cluster hosted by the Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany. One instance was for testing and development, the second for production use within the project, and the third was a reduced system that could be made available to the public for demonstration. In all cases, the deployment worked without problems and required only little changes to the Ansible playbooks.

Since our system does not have any specific requirements, it could in fact be transferred to other environments too. The target platform only has to have some kind of support for virtual machines on which we can install our services with Ansible. The individual components of our system are also platform-independent. Most of the core services run in the Java Virtual Machine. The processing services have been containerised with Docker and packed into images that bring their own operating system, file system, dependencies and configuration.

[[sec:evaluation-stakeholder-requirements]]
== Stakeholder requirements

In the following we review the stakeholder requirements defined in Section <<sec:architecture-stakeholders>>. We summarise the demands from the people (or roles) who have an interest in our system and discuss to which degree these demands are satisfied.

=== Users (GIS experts)

GIS experts need to work with large geospatial data sets, but their local workstations often lack processing power and storage capabilities. Our system gives them access to the Cloud and therefore virtually unlimited storage space and computational power. It also allows them to share their data sets with colleagues from other departments or even with other institutions or authorities.

GIS experts are used to desktop GIS solutions that offer a number of spatial operations and processing algorithms. Our microservice architecture allows a wide range of processing services to be integrated. These services can cover the functionality of a desktop GIS. Our system can therefore be considered a Cloud-based GIS.

With the workflow management and editing capabilities of our system, the GIS experts can specify how their data should be processed. In typical desktop GIS solutions they have to deal with general-purpose programming languages. The Domain-Specific Language we presented in this work allows them to focus on the workflow definition without requiring deep knowledge of programming. Our system hides technical details about the infrastructure the defined workflows are executed on and transparently generates a strategy to make best use of available resources.

The Domain-Specific Language we created in Chapter <<chap:workflow-modelling>> covers our use cases from Section <<sec:introduction-use-cases>> but also other scenarios. In the IQmulus research project we used the same DSL to implement workflows for nine different use cases from the land, urban or marine domain. The language itself only contains a few generic keywords but can be extended by registering additional processing services. The human-readable names from the service metadata become part of the language. New services can directly be used via the generic '`apply`' keyword. If more high-level language constructs are required, the language grammar has to be extended.

We designed the language to be easy to use and to hide technical details related to the infrastructure or the fact that services are potentially run in parallel in a distributed system. For example, our language only contains immutable variables (constants). This avoids many pitfalls of concurrent programming such as shared write access to resources. The limited expressiveness makes it easier for users to learn and understand our language, as well as to avoid technical details of distributed computing. However, it can--at least at first glance--also limit the system's use for very specific cases. For example, in the IQmulus research project we had a case where a user was struggling with the definition of a suitable workflow. The user was used to general-purpose languages and missed features such as arbitrary loops or mutable variables. After analysing the requirements thoroughly, we were able to implement the workflow with our language but the initial learning curve was very high for this user.

Nevertheless, our language has proven to be very efficient in practise and we could cover many use cases. Even though this thesis is about the processing of geospatial data, only a few elements of our language are related to this domain. In fact, we believe, due to its modularity based on service metadata, the same language could be used for use cases from other domains as well. In any case, the method for DSL modelling we presented in Chapter <<chap:workflow-modelling>> can be used to create a new language or to modify ours and to adapt it to specific requirements.

=== Users (Data providers)

Data providers have similar overall requirements as GIS experts but typically need to deal with different types of data sets. They also often acquire large amounts of data that they quickly need to pre-process and deliver to their customers. The Cloud allows these users to scale out and to add new computational resources if required. We have shown that our system is scalable and that it makes best use of additional resources. With our urban modelling use case, we have also shown that--with enough computational power--we can process data faster than it was acquired.

Compared to other end-users, data providers often require a different set of processing algorithms. Our system is very modular and the functionality of an instance of our system depends on the deployed processing services. Our system can be installed in various configurations targeting different user groups. This allows us to cover requirements from many use cases. Although it is optimised for geospatial data processing, our system could in general also be used outside the geospatial domain.

Data providers often need to perform the same tasks multiple times. For example, regularly acquired LiDAR data sets need to be processed always in the same way. Our system allows data providers to define workflows once and later re-use them many times. In this respect, our Domain-Specific Language helps them create workflows, but more importantly it allows them to understand existing workflows and to decide whether they are suitable or not. This is particularly important if they need to deal with a large number of pre-defined workflows from which they select a specific one that fits a certain data set or use case.

One drawback of the approach to process geospatial data in the Cloud is that data is given away to an external infrastructure provider that needs to be considered honest but curious cite:kraemer-frese-2017[]. Geospatial data may contain confidential information (e.g. about public infrastructure or about citizens) that should not be stored or processed in a public Cloud without additional security measures preventing unauthorised access. Data providers need to be sure that their data, which is the foundation of their business, cannot be stolen by third parties. The fact that storing data in the Cloud is potentially insecure is, however, not a drawback of our work, but of the concept of the Cloud in general. As mentioned earlier, a comprehensive security concept addressing this issue is beyond the scope of this thesis. We refer to our work on secure Cloud-based storage for geospatial data, which we conducted in parallel with this thesis cite:hiemenz-kraemer-2017[].

=== Members of the system development team

The members of the system development team aim for creating a system that is maintainable and extensible. In Section <<sec:evaluation-modifiability>> we have shown that new functionality can be easily added to our system and that the isolation between our microservices can help find and fix bugs. The microservice architectural style even allows individual components to be redeployed while the system is running. This enables continuous integration and hence gives end-users direct access to new functionality and other improvements.

A microservice architecture can, however, become very complex to maintain. Compared to a monolithic application, the sheer number of individual components that are deployed to various distributed nodes in the Cloud can make it hard to keep an overview. As described in Section&nbsp;<<sec:architecture-microservice-architectures>>, dividing the services into bounded contexts can help tackling the complexity. Regarding operations, in our system we use monitoring and distributed logging to observe the state of our services and the infrastructure. We also use IT automation for deployment and to maintain an overview of the services and the nodes they run on. We apply the _Infrastructure as Code (IaC)_ approach (see Section <<sec:architecture-infrastructure-deployment>>) to be able to trace back changes to our system and its configuration.

=== Developers of spatial processing algorithms

Developers of spatial processing algorithms (or processing services) are often people with different backgrounds (such as mathematics, geography, or physics) who have little knowledge of distributed programming. Our architecture provides these developers with guidelines how to develop processing services that can be safely executed concurrently in a distributed environment (see Section <<sec:architecture-processing-service-requirements>>).

Our architecture allows a wide range of processing services to be integrated and to be executed in parallel. This even applies to single-threaded algorithms that have not been developed for the Cloud. With our architecture developers are able to reuse existing services they created earlier and into which they have already put a lot of knowledge and effort. These services can be integrated without fundamental modifications.

As shown in Section <<sec:evaluation-development-distributability>>, the microservice architectural style allows distributed teams of developers to create individual software components (including processing services) and to integrate them at a central location. This enables many parties to contribute their knowledge to our system and to reasonably extend its functionality.

Coordinating such a distributed development effort can, however, also be very complex. It requires regular communication so that all contributing teams are aware of architectural specifications such as the guidelines for processing services described in Section <<sec:architecture-processing-service-requirements>>. It also requires that these specifications are actually followed. For example, the fact that processing services need to be idempotent is a key concept of our architecture. If the results generated by the services are not reproducible, our mechanisms for fault tolerance will not work reliably.

Furthermore, the distributed development approach requires high discipline with regards to the quality of software artefacts and at which point they can be considered releasable or deployable. According to the "`You build it, you run it`" principle cite:OHanlon2006[] contributing teams therefore become responsible for the whole lifecycle of their software artefacts including development, testing, deployment and support. Despite the additional complexity and efforts, this way of development opens new possibilities. Independent teams can be a lot more agile and release new versions of their components when they think they are ready, without needing to comply with an artificial release schedule that applies to the whole distributed application. By providing support for their components, the developers get into direct contact with the end-users and can therefore create software that better satisfies their requirements. This helps reduce uncertainty and improves the overall project result. Finally, the microservice architectural style encourages teams to use different technologies that suit their specific skills and requirements best, instead of needing to rely on a technology stack that applies to the whole distributed application. This can reduce the learning curve developers have when they first join a project.

=== Integrators

Integrators are responsible for connecting core system components and processing services to a running application. To this end, they require the software artefacts to be stored in a central repository and to follow a unified version number scheme. They also require well-defined interfaces or at least machine-readable interface descriptions such as the metadata for our processing services.

As described above, within the IQmulus research project we integrated more than a hundred microservices. We used the software artefact repository Artifactory to store binaries of all system components and processing services in multiple versions. We used the semantic versioning scheme to be able to tell if an update of a software artefact was compatible to the previous version or not. This helped us reduce the integration effort since we could avoid thorough compatibility tests when we needed to update individual components. However, it also required high discipline from the developers to assign correct version numbers. Although the semantic versioning scheme has become common practise in industry in recent years, many of the processing service developers we worked with had no background in computer science and did not know this scheme. In these cases, it was important to communicate why such a scheme is required and what could potentially go wrong if they did not comply with it.

The possibility to integrate arbitrary processing services by describing their interface in a simple JSON file (i.e. service metadata) was beneficial for us. We were able to reuse existing algorithms and services without requiring fundamental modifications. This allowed to us to extend the overall functionality of the system quickly without having to develop basic processing algorithms from scratch. Compared to a monolithic application where everything is already integrated at the moment the code is uploaded to the central source repository, integrating loosely coupled services can however be very complex. As mentioned in Section <<sec:architecture-stakeholders>>, all developers should be directly involved in integrating their components in order to better understand the specific requirements of a distributed environment.

Another notable aspect that helps integrate and operate a large number of services is containerisation. As described earlier, our processing services run in Docker containers, which isolates them from other software running on the same virtual machine. It also allows processing services to depend on different Linux versions or libraries, without getting into conflict with each other.

=== Testers

As described in Section <<sec:architecture-stakeholders>>, the microservice architectural style allows individual parts of the system (i.e. the services) to be tested separately. This can help identify issues early on before the services are integrated into the system. However, a microservice architecture also requires tests at other levels (e.g. integration tests or end-to-end tests). Even if there are thorough tests for a single service, it does not mean the service will behave as expected when it is integrated into the system and needs to communicate with other services. In addition, in order to be sure that the distributed application satisfies the needs of the end-users, UI tests or acceptance tests are required.

Compared to a monolithic application, testing can become very complex and time-consuming in a microservice architecture. It is important to find the right balance between test coverage and the level of confidence one can achieve with testing. As described in Section <<sec:architecture-other-quality-attributes>>, a thorough concept for testing a microservice architecture such as ours is beyond the scope of this work. For more information on this topic we refer to citet:newman2015[suffix=", Chapter 7"]. Nevertheless, from our experience from the IQmulus research project, we can state that testing requires high discipline in a microservice architecture and many developers prefer to focus on implementing features than on making sure each and every corner case is covered. One of the benefits of the microservice architectural style is that new functionality has a short time to market, which means it can be implemented and provided to customers quickly. In order to ensure the stability of the integrated system is acceptable without slowing down the delivery process, many companies go a different path and do not put too much effort into testing. Instead, they implement a highly automated deployment pipeline that allows them to quickly revert service updates if a problem occurs in production using strategies such as _Canary Releases_ or _Blue-Green Deployments_ cite:Humble2010[].

=== IT operations

The IT operations group is responsible for deploying an integrated system into production and to ensure continuous operation. To this end, they need automated deployment processes as well as means to monitor the system and the infrastructure.

As described above, we have used the IT automation tool Ansible to automatically deploy our system to production. This has worked very well and allowed us to make updates up to several times per day in a short time. The microservice architectural style additionally enabled us to deploy individual parts of our system separately without having to restart the whole system (_Zero-Downtime Releases_). Key to this is that everything needs to be automated, no configurations on the virtual machines are changed manually, and no software artefacts are installed without using the IT automation tool. This requires discipline from the IT operations group and the developers and can be enforced by disallowing manual SSH access to the virtual machines.

In order to monitor the system and the infrastructure, we used the tools Prometheus and Grafana. In this chapter we presented several figures that have been created with these tools. Monitoring was important for us in two ways. It helped us during development to optimise the system so that it makes best use of available Cloud resources. In addition, it is key to a continuous and smooth operation. Grafana allows for creating a dashboard where all important metrics are directly available. Through configurable alerts one can be notified about problems immediately in order to be able to react in a short time. The same applies to distributed logging. As described above we deployed Logstash, Elasticsearch and Kibana to be able to analyse log files of distributed services at a central place and to get immediate notifications about possible issues.

=== Business professionals

In Section <<sec:architecture-stakeholders>> we differentiated between two types of people who have a business interest in our system: system resellers and managers of GIS projects. Both have similar requirements regarding the quality of the results our system produces, the time it takes to generate the results, and the amount of human interaction involved. They are also concerned about costs, in particular regarding maintenance and operations.

We have shown that our system is capable of running pre-defined workflows in an automated way without requiring human interaction, other than selecting the data sets to process and setting initial parameters for the processing services. We have also shown that our system is scalable and that additional computational resources can decrease the time it takes to process large data sets. The quality of the results depends on the individual processing services. Due to the microservice architecture, they can be easily replaced by improved algorithms in the future, if the current quality should not suffice.

Regarding maintenance and operations, we have shown that due to the high degree of automation we employ, the effort to update the system and to ensure smooth operation is kept at a reasonable level. The microservice architecture also allows new functionality to be added and updates to be deployed without affecting the rest of the system. Business professionals require a short time to market, which our system can offer as described above.

== Objectives of the thesis

In this section we evaluate our solution against the objectives we defined at the beginning of our thesis in Section <<sec:introduction-objectives>>. The aim of this is to validate that our system is able to satisfy the general requirements from the two user groups from our problem statement in Section <<sec:introduction-problem-statement>>. In order to do so, we first relate the stakeholder requirements to our quality attributes (see Table <<tab:mapping-stakeholders-to-quality-attributes>>).

[[tab:mapping-stakeholders-to-quality-attributes]]
.A matrix which shows the relation between stakeholders and quality attributes
[cols="30,^.^10a,^.^10a,^.^10a,^.^10a,^.^10a,^.^10a,^.^10a",frame=none,grid=all,options="header"]
|===
|
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Performance</fo:block></fo:block-container></passthrough>]
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Scalability</fo:block></fo:block-container></passthrough>]
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Availability</fo:block></fo:block-container></passthrough>]
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Modifiability</fo:block></fo:block-container></passthrough>]
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Development distributability</fo:block></fo:block-container></passthrough>]
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Deployability</fo:block></fo:block-container></passthrough>]
| pass:[<passthrough xmlns:fo="http://www.w3.org/1999/XSL/Format"><fo:block-container reference-orientation="-90" display-align="center" inline-progression-dimension.minimum="1mm" inline-progression-dimension.optimum="30mm" inline-progression-dimension.maximum="auto"><fo:block hyphenate="true">Portability</fo:block></fo:block-container></passthrough>]

| Users (GIS experts and data providers)
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
|
|
|

| System development team
| 
|
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
|

| Developers of spatial processing algorithms
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
|

| Integrators
|
|
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]

| Testers
|
|
|
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
|

| IT operations
|
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]

| Business professionals
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
| image::images/check_full.svg[scaledwidth="10pt",align="center"]

|===

The table is directly derived from the stakeholder requirements defined in Section <<sec:architecture-stakeholders>>. It shows to what extent the individual stakeholders require a specific quality attribute of our system. The circle filled with black image:images/check_full.svg[height="10pt"] means the quality attribute is a primary concern, and the white circle image:images/check_normal.svg[height="10pt"] indicates a secondary concern.

From this table we can deduce that all quality attributes of our architecture map to stakeholders. The evaluation results from the previous sections show that in our system implementation all quality attributes are fulfilled and that the stakeholder requirements are satisfied.

We now map stakeholders to the two user groups from our problem statement in Section&nbsp;<<sec:introduction-problem-statement>>, namely GIS users and developers/researchers creating spatial processing algorithms (see Table&nbsp;<<tab:mapping-stakeholders-to-problem-statement>>).


[[tab:mapping-stakeholders-to-problem-statement]]
.A matrix which maps stakeholders to user groups from our problem statement
[cols="30,^.^35a,^.^35a",frame=none,grid=all,options="header"]
|===
|
| GIS users
| Developers/researchers

| Users (GIS experts and data providers)
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
|

| System development team
| 
| image::images/check_full.svg[scaledwidth="10pt",align="center"]

| Developers of spatial processing algorithms
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]

| Integrators
|
| image::images/check_full.svg[scaledwidth="10pt",align="center"]

| Testers
| image::images/check_full.svg[scaledwidth="10pt",align="center"]
|

| IT operations
|
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]

| Business professionals
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]
| image::images/check_normal.svg[scaledwidth="10pt",align="center"]

|===

The black circle image:images/check_full.svg[height="10pt"] means the stakeholder role can be directly mapped to one of the user groups. The white circle image:images/check_normal.svg[height="10pt"] indicates that a stakeholder can only be partly mapped to a user group or mapped to both groups.

These tables show that the two user groups defined in the objectives of our thesis can be mapped to stakeholders, and that all stakeholder requirements are satisfied by our system. _In consequence, we can now deduce that the requirements from our two user groups are satisfied and that the objectives of our thesis have been reached._ We created a system that has a user interface to process large geospatial data in the Cloud without requiring users to have expertise in distributed computing. The system is extensible so that it can cover the same functionality as a desktop GIS. It also offers the possibility to execute workflows. Existing processing algorithms can be integrated through a generic interface. The system can deploy, orchestrate and parallelise the services without requiring the developers to have expertise in Cloud Computing or to redesign their algorithms.

== Summary

In this chapter we have presented the results from evaluating our architecture and its implementation. We have performed a quantitative evaluation where we first defined specific scenarios for each quality attribute our system should have, and then validated if the system satisfies the criteria given in the scenarios. We then revisited the stakeholder requirements formulated in Chapter <<chap:architecture>> and discussed in which way our system meets them. We also evaluated whether the main objectives of the thesis and the general requirements from the two user groups from our problem statement are met.

Our quantitative evaluation was based on two use cases representing real-world problems from the areas of urban planning and land monitoring. We were able to show that our architecture is suitable to execute workflows from these use cases. A notable result is that the goal of the urban planning use case could be reached. We were indeed able to process large 3D point clouds faster than they were acquired. This allows municipalities and mapping authorities to efficiently utilise the data collected by laser mobile mapping systems (LMMS) and to perform tasks such as environmental monitoring based on up-to-date information. Regarding the land monitoring use case, we were able to process a large geospatial data set in the Cloud within a short amount of time. As described in Section <<sec:introduction-use-cases>>, users from the Liguria Region reported that it took them several days to process the data on their workstations, but with our system, as shown in Section&nbsp;<<sec:evaluation-use-case-b>>, it only takes about 35 minutes. This opens new possibilities for the users as it allows them to perform data analysis faster and to better prepare against environmental catastrophes.

In summary, the results of our evaluation were positive. We were able to show that our system meets all requirements in terms of quality attributes and stakeholder needs. In the next chapter we follow up on this and present conclusions and future research perspectives for our work.
